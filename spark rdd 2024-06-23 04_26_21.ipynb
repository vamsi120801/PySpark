{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15991212-4e1f-4d90-ac2c-bf191d2cc99d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Creation of RDD from list in program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "335454d8-ee09-4ab1-af4c-37a24688326b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "rdd_1 = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eecf0ce-c839-47d5-b651-14edae41f31c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">the rdd_1 is [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
       "the sum of rdd_1 is 210\n",
       "the max of rdd_1 is 20\n",
       "the min of rdd_1 is 1\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">the rdd_1 is [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\nthe sum of rdd_1 is 210\nthe max of rdd_1 is 20\nthe min of rdd_1 is 1\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"the rdd_1 is\",rdd_1.collect())\n",
    "print(\"the sum of rdd_1 is\",rdd_1.sum())\n",
    "print(\"the max of rdd_1 is\",rdd_1.max())\n",
    "print(\"the min of rdd_1 is\", rdd_1.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49f32607-2841-4304-8d0f-e68885394f47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "list = ['azure', 'aws', 'gcp', 'gcp','aws', 'azure', 'google cloud']\n",
    "x = sc.parallelize(list)\n",
    "y = x.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "371f293a-97dd-43a9-b97e-1ec618054997",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">the complete list is [&#39;azure&#39;, &#39;aws&#39;, &#39;gcp&#39;, &#39;gcp&#39;, &#39;aws&#39;, &#39;azure&#39;, &#39;google cloud&#39;]\n",
       "the unique list is [&#39;azure&#39;, &#39;aws&#39;, &#39;google cloud&#39;, &#39;gcp&#39;]\n",
       "the number of unique list is 4\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">the complete list is [&#39;azure&#39;, &#39;aws&#39;, &#39;gcp&#39;, &#39;gcp&#39;, &#39;aws&#39;, &#39;azure&#39;, &#39;google cloud&#39;]\nthe unique list is [&#39;azure&#39;, &#39;aws&#39;, &#39;google cloud&#39;, &#39;gcp&#39;]\nthe number of unique list is 4\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"the complete list is\", x.collect())\n",
    "print(\"the unique list is\", y.collect())\n",
    "print(\"the number of unique list is\", y.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeb51a79-787c-4a32-80d0-997fe1cddeef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">the type of x is &lt;class &#39;pyspark.rdd.RDD&#39;&gt;\n",
       "the type of y is &lt;class &#39;pyspark.rdd.PipelinedRDD&#39;&gt;\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">the type of x is &lt;class &#39;pyspark.rdd.RDD&#39;&gt;\nthe type of y is &lt;class &#39;pyspark.rdd.PipelinedRDD&#39;&gt;\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"the type of x is\", type(x))\n",
    "print(\"the type of y is\", type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a2ecb74-4397-4016-9311-ff31da0002cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[10]: [&#39;__add__&#39;,\n",
       " &#39;__class__&#39;,\n",
       " &#39;__delattr__&#39;,\n",
       " &#39;__dict__&#39;,\n",
       " &#39;__dir__&#39;,\n",
       " &#39;__doc__&#39;,\n",
       " &#39;__eq__&#39;,\n",
       " &#39;__format__&#39;,\n",
       " &#39;__ge__&#39;,\n",
       " &#39;__getattribute__&#39;,\n",
       " &#39;__getnewargs__&#39;,\n",
       " &#39;__gt__&#39;,\n",
       " &#39;__hash__&#39;,\n",
       " &#39;__init__&#39;,\n",
       " &#39;__init_subclass__&#39;,\n",
       " &#39;__le__&#39;,\n",
       " &#39;__lt__&#39;,\n",
       " &#39;__module__&#39;,\n",
       " &#39;__ne__&#39;,\n",
       " &#39;__new__&#39;,\n",
       " &#39;__reduce__&#39;,\n",
       " &#39;__reduce_ex__&#39;,\n",
       " &#39;__repr__&#39;,\n",
       " &#39;__setattr__&#39;,\n",
       " &#39;__sizeof__&#39;,\n",
       " &#39;__str__&#39;,\n",
       " &#39;__subclasshook__&#39;,\n",
       " &#39;__weakref__&#39;,\n",
       " &#39;_computeFractionForSampleSize&#39;,\n",
       " &#39;_defaultReducePartitions&#39;,\n",
       " &#39;_id&#39;,\n",
       " &#39;_is_barrier&#39;,\n",
       " &#39;_jrdd&#39;,\n",
       " &#39;_jrdd_deserializer&#39;,\n",
       " &#39;_memory_limit&#39;,\n",
       " &#39;_pickled&#39;,\n",
       " &#39;_reserialize&#39;,\n",
       " &#39;_to_java_object_rdd&#39;,\n",
       " &#39;aggregate&#39;,\n",
       " &#39;aggregateByKey&#39;,\n",
       " &#39;barrier&#39;,\n",
       " &#39;cache&#39;,\n",
       " &#39;cartesian&#39;,\n",
       " &#39;checkpoint&#39;,\n",
       " &#39;coalesce&#39;,\n",
       " &#39;cogroup&#39;,\n",
       " &#39;collect&#39;,\n",
       " &#39;collectAsMap&#39;,\n",
       " &#39;collectWithJobGroup&#39;,\n",
       " &#39;combineByKey&#39;,\n",
       " &#39;context&#39;,\n",
       " &#39;count&#39;,\n",
       " &#39;countApprox&#39;,\n",
       " &#39;countApproxDistinct&#39;,\n",
       " &#39;countByKey&#39;,\n",
       " &#39;countByValue&#39;,\n",
       " &#39;ctx&#39;,\n",
       " &#39;distinct&#39;,\n",
       " &#39;filter&#39;,\n",
       " &#39;first&#39;,\n",
       " &#39;flatMap&#39;,\n",
       " &#39;flatMapValues&#39;,\n",
       " &#39;fold&#39;,\n",
       " &#39;foldByKey&#39;,\n",
       " &#39;foreach&#39;,\n",
       " &#39;foreachPartition&#39;,\n",
       " &#39;fullOuterJoin&#39;,\n",
       " &#39;getCheckpointFile&#39;,\n",
       " &#39;getNumPartitions&#39;,\n",
       " &#39;getResourceProfile&#39;,\n",
       " &#39;getStorageLevel&#39;,\n",
       " &#39;glom&#39;,\n",
       " &#39;groupBy&#39;,\n",
       " &#39;groupByKey&#39;,\n",
       " &#39;groupWith&#39;,\n",
       " &#39;has_resource_profile&#39;,\n",
       " &#39;histogram&#39;,\n",
       " &#39;id&#39;,\n",
       " &#39;intersection&#39;,\n",
       " &#39;isCheckpointed&#39;,\n",
       " &#39;isEmpty&#39;,\n",
       " &#39;isLocallyCheckpointed&#39;,\n",
       " &#39;is_cached&#39;,\n",
       " &#39;is_checkpointed&#39;,\n",
       " &#39;join&#39;,\n",
       " &#39;keyBy&#39;,\n",
       " &#39;keys&#39;,\n",
       " &#39;leftOuterJoin&#39;,\n",
       " &#39;localCheckpoint&#39;,\n",
       " &#39;lookup&#39;,\n",
       " &#39;map&#39;,\n",
       " &#39;mapPartitions&#39;,\n",
       " &#39;mapPartitionsWithIndex&#39;,\n",
       " &#39;mapPartitionsWithSplit&#39;,\n",
       " &#39;mapValues&#39;,\n",
       " &#39;max&#39;,\n",
       " &#39;mean&#39;,\n",
       " &#39;meanApprox&#39;,\n",
       " &#39;min&#39;,\n",
       " &#39;name&#39;,\n",
       " &#39;partitionBy&#39;,\n",
       " &#39;partitioner&#39;,\n",
       " &#39;persist&#39;,\n",
       " &#39;pipe&#39;,\n",
       " &#39;randomSplit&#39;,\n",
       " &#39;reduce&#39;,\n",
       " &#39;reduceByKey&#39;,\n",
       " &#39;reduceByKeyLocally&#39;,\n",
       " &#39;repartition&#39;,\n",
       " &#39;repartitionAndSortWithinPartitions&#39;,\n",
       " &#39;rightOuterJoin&#39;,\n",
       " &#39;sample&#39;,\n",
       " &#39;sampleByKey&#39;,\n",
       " &#39;sampleStdev&#39;,\n",
       " &#39;sampleVariance&#39;,\n",
       " &#39;saveAsHadoopDataset&#39;,\n",
       " &#39;saveAsHadoopFile&#39;,\n",
       " &#39;saveAsNewAPIHadoopDataset&#39;,\n",
       " &#39;saveAsNewAPIHadoopFile&#39;,\n",
       " &#39;saveAsPickleFile&#39;,\n",
       " &#39;saveAsSequenceFile&#39;,\n",
       " &#39;saveAsTextFile&#39;,\n",
       " &#39;setName&#39;,\n",
       " &#39;sortBy&#39;,\n",
       " &#39;sortByKey&#39;,\n",
       " &#39;stats&#39;,\n",
       " &#39;stdev&#39;,\n",
       " &#39;subtract&#39;,\n",
       " &#39;subtractByKey&#39;,\n",
       " &#39;sum&#39;,\n",
       " &#39;sumApprox&#39;,\n",
       " &#39;take&#39;,\n",
       " &#39;takeOrdered&#39;,\n",
       " &#39;takeSample&#39;,\n",
       " &#39;toDF&#39;,\n",
       " &#39;toDebugString&#39;,\n",
       " &#39;toLocalIterator&#39;,\n",
       " &#39;top&#39;,\n",
       " &#39;treeAggregate&#39;,\n",
       " &#39;treeReduce&#39;,\n",
       " &#39;union&#39;,\n",
       " &#39;unpersist&#39;,\n",
       " &#39;values&#39;,\n",
       " &#39;variance&#39;,\n",
       " &#39;withResources&#39;,\n",
       " &#39;zip&#39;,\n",
       " &#39;zipWithIndex&#39;,\n",
       " &#39;zipWithUniqueId&#39;]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[10]: [&#39;__add__&#39;,\n &#39;__class__&#39;,\n &#39;__delattr__&#39;,\n &#39;__dict__&#39;,\n &#39;__dir__&#39;,\n &#39;__doc__&#39;,\n &#39;__eq__&#39;,\n &#39;__format__&#39;,\n &#39;__ge__&#39;,\n &#39;__getattribute__&#39;,\n &#39;__getnewargs__&#39;,\n &#39;__gt__&#39;,\n &#39;__hash__&#39;,\n &#39;__init__&#39;,\n &#39;__init_subclass__&#39;,\n &#39;__le__&#39;,\n &#39;__lt__&#39;,\n &#39;__module__&#39;,\n &#39;__ne__&#39;,\n &#39;__new__&#39;,\n &#39;__reduce__&#39;,\n &#39;__reduce_ex__&#39;,\n &#39;__repr__&#39;,\n &#39;__setattr__&#39;,\n &#39;__sizeof__&#39;,\n &#39;__str__&#39;,\n &#39;__subclasshook__&#39;,\n &#39;__weakref__&#39;,\n &#39;_computeFractionForSampleSize&#39;,\n &#39;_defaultReducePartitions&#39;,\n &#39;_id&#39;,\n &#39;_is_barrier&#39;,\n &#39;_jrdd&#39;,\n &#39;_jrdd_deserializer&#39;,\n &#39;_memory_limit&#39;,\n &#39;_pickled&#39;,\n &#39;_reserialize&#39;,\n &#39;_to_java_object_rdd&#39;,\n &#39;aggregate&#39;,\n &#39;aggregateByKey&#39;,\n &#39;barrier&#39;,\n &#39;cache&#39;,\n &#39;cartesian&#39;,\n &#39;checkpoint&#39;,\n &#39;coalesce&#39;,\n &#39;cogroup&#39;,\n &#39;collect&#39;,\n &#39;collectAsMap&#39;,\n &#39;collectWithJobGroup&#39;,\n &#39;combineByKey&#39;,\n &#39;context&#39;,\n &#39;count&#39;,\n &#39;countApprox&#39;,\n &#39;countApproxDistinct&#39;,\n &#39;countByKey&#39;,\n &#39;countByValue&#39;,\n &#39;ctx&#39;,\n &#39;distinct&#39;,\n &#39;filter&#39;,\n &#39;first&#39;,\n &#39;flatMap&#39;,\n &#39;flatMapValues&#39;,\n &#39;fold&#39;,\n &#39;foldByKey&#39;,\n &#39;foreach&#39;,\n &#39;foreachPartition&#39;,\n &#39;fullOuterJoin&#39;,\n &#39;getCheckpointFile&#39;,\n &#39;getNumPartitions&#39;,\n &#39;getResourceProfile&#39;,\n &#39;getStorageLevel&#39;,\n &#39;glom&#39;,\n &#39;groupBy&#39;,\n &#39;groupByKey&#39;,\n &#39;groupWith&#39;,\n &#39;has_resource_profile&#39;,\n &#39;histogram&#39;,\n &#39;id&#39;,\n &#39;intersection&#39;,\n &#39;isCheckpointed&#39;,\n &#39;isEmpty&#39;,\n &#39;isLocallyCheckpointed&#39;,\n &#39;is_cached&#39;,\n &#39;is_checkpointed&#39;,\n &#39;join&#39;,\n &#39;keyBy&#39;,\n &#39;keys&#39;,\n &#39;leftOuterJoin&#39;,\n &#39;localCheckpoint&#39;,\n &#39;lookup&#39;,\n &#39;map&#39;,\n &#39;mapPartitions&#39;,\n &#39;mapPartitionsWithIndex&#39;,\n &#39;mapPartitionsWithSplit&#39;,\n &#39;mapValues&#39;,\n &#39;max&#39;,\n &#39;mean&#39;,\n &#39;meanApprox&#39;,\n &#39;min&#39;,\n &#39;name&#39;,\n &#39;partitionBy&#39;,\n &#39;partitioner&#39;,\n &#39;persist&#39;,\n &#39;pipe&#39;,\n &#39;randomSplit&#39;,\n &#39;reduce&#39;,\n &#39;reduceByKey&#39;,\n &#39;reduceByKeyLocally&#39;,\n &#39;repartition&#39;,\n &#39;repartitionAndSortWithinPartitions&#39;,\n &#39;rightOuterJoin&#39;,\n &#39;sample&#39;,\n &#39;sampleByKey&#39;,\n &#39;sampleStdev&#39;,\n &#39;sampleVariance&#39;,\n &#39;saveAsHadoopDataset&#39;,\n &#39;saveAsHadoopFile&#39;,\n &#39;saveAsNewAPIHadoopDataset&#39;,\n &#39;saveAsNewAPIHadoopFile&#39;,\n &#39;saveAsPickleFile&#39;,\n &#39;saveAsSequenceFile&#39;,\n &#39;saveAsTextFile&#39;,\n &#39;setName&#39;,\n &#39;sortBy&#39;,\n &#39;sortByKey&#39;,\n &#39;stats&#39;,\n &#39;stdev&#39;,\n &#39;subtract&#39;,\n &#39;subtractByKey&#39;,\n &#39;sum&#39;,\n &#39;sumApprox&#39;,\n &#39;take&#39;,\n &#39;takeOrdered&#39;,\n &#39;takeSample&#39;,\n &#39;toDF&#39;,\n &#39;toDebugString&#39;,\n &#39;toLocalIterator&#39;,\n &#39;top&#39;,\n &#39;treeAggregate&#39;,\n &#39;treeReduce&#39;,\n &#39;union&#39;,\n &#39;unpersist&#39;,\n &#39;values&#39;,\n &#39;variance&#39;,\n &#39;withResources&#39;,\n &#39;zip&#39;,\n &#39;zipWithIndex&#39;,\n &#39;zipWithUniqueId&#39;]</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce82b171-139a-4f32-8606-ff0553d682ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Help on RDD in module pyspark.rdd object:\n",
       "\n",
       "class RDD(builtins.object)\n",
       "  RDD(jrdd, ctx, jrdd_deserializer=AutoBatchedSerializer(PickleSerializer()))\n",
       "  \n",
       "  A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n",
       "  Represents an immutable, partitioned collection of elements that can be\n",
       "  operated on in parallel.\n",
       "  \n",
       "  Methods defined here:\n",
       "  \n",
       "  __add__(self, other)\n",
       "      Return the union of this RDD and another one.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; rdd = sc.parallelize([1, 1, 2, 3])\n",
       "      &gt;&gt;&gt; (rdd + rdd).collect()\n",
       "      [1, 1, 2, 3, 1, 1, 2, 3]\n",
       "  \n",
       "  __getnewargs__(self)\n",
       "  \n",
       "  __init__(self, jrdd, ctx, jrdd_deserializer=AutoBatchedSerializer(PickleSerializer()))\n",
       "      Initialize self.  See help(type(self)) for accurate signature.\n",
       "  \n",
       "  __repr__(self)\n",
       "      Return repr(self).\n",
       "  \n",
       "  aggregate(self, zeroValue, seqOp, combOp)\n",
       "      Aggregate the elements of each partition, and then the results for all\n",
       "      the partitions, using a given combine functions and a neutral &#34;zero\n",
       "      value.&#34;\n",
       "      \n",
       "      The functions ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n",
       "      as its result value to avoid object allocation; however, it should not\n",
       "      modify ``t2``.\n",
       "      \n",
       "      The first function (seqOp) can return a different result type, U, than\n",
       "      the type of this RDD. Thus, we need one operation for merging a T into\n",
       "      an U and one operation for merging two U\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
       "      &gt;&gt;&gt; combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
       "      &gt;&gt;&gt; sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n",
       "      (10, 4)\n",
       "      &gt;&gt;&gt; sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n",
       "      (0, 0)\n",
       "  \n",
       "  aggregateByKey(self, zeroValue, seqFunc, combFunc, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f9589109670&gt;)\n",
       "      Aggregate the values of each key, using given combine functions and a neutral\n",
       "      &#34;zero value&#34;. This function can return a different result type, U, than the type\n",
       "      of the values in this RDD, V. Thus, we need one operation for merging a V into\n",
       "      a U and one operation for merging two U&#39;s, The former operation is used for merging\n",
       "      values within a partition, and the latter is used for merging values between\n",
       "      partitions. To avoid memory allocation, both of these functions are\n",
       "      allowed to modify and return their first argument instead of creating a new U.\n",
       "  \n",
       "  barrier(self)\n",
       "      Marks the current stage as a barrier stage, where Spark must launch all tasks together.\n",
       "      In case of a task failure, instead of only restarting the failed task, Spark will abort the\n",
       "      entire stage and relaunch all tasks for this stage.\n",
       "      The barrier execution mode feature is experimental and it only handles limited scenarios.\n",
       "      Please read the linked SPIP and design docs to understand the limitations and future plans.\n",
       "      \n",
       "      .. versionadded:: 2.4.0\n",
       "      \n",
       "      Returns\n",
       "      -------\n",
       "      :class:`RDDBarrier`\n",
       "          instance that provides actions within a barrier stage.\n",
       "      \n",
       "      See Also\n",
       "      --------\n",
       "      pyspark.BarrierTaskContext\n",
       "      \n",
       "      Notes\n",
       "      -----\n",
       "      For additional information see\n",
       "      \n",
       "      - `SPIP: Barrier Execution Mode &lt;http://jira.apache.org/jira/browse/SPARK-24374&gt;`_\n",
       "      - `Design Doc &lt;https://jira.apache.org/jira/browse/SPARK-24582&gt;`_\n",
       "      \n",
       "      This API is experimental\n",
       "  \n",
       "  cache(self)\n",
       "      Persist this RDD with the default storage level (`MEMORY_ONLY`).\n",
       "  \n",
       "  cartesian(self, other)\n",
       "      Return the Cartesian product of this RDD and another one, that is, the\n",
       "      RDD of all pairs of elements ``(a, b)`` where ``a`` is in `self` and\n",
       "      ``b`` is in `other`.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; rdd = sc.parallelize([1, 2])\n",
       "      &gt;&gt;&gt; sorted(rdd.cartesian(rdd).collect())\n",
       "      [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
       "  \n",
       "  checkpoint(self)\n",
       "      Mark this RDD for checkpointing. It will be saved to a file inside the\n",
       "      checkpoint directory set with :meth:`SparkContext.setCheckpointDir` and\n",
       "      all references to its parent RDDs will be removed. This function must\n",
       "      be called before any job has been executed on this RDD. It is strongly\n",
       "      recommended that this RDD is persisted in memory, otherwise saving it\n",
       "      on a file will require recomputation.\n",
       "  \n",
       "  coalesce(self, numPartitions, shuffle=False)\n",
       "      Return a new RDD that is reduced into `numPartitions` partitions.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n",
       "      [[1], [2, 3], [4, 5]]\n",
       "      &gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n",
       "      [[1, 2, 3, 4, 5]]\n",
       "  \n",
       "  cogroup(self, other, numPartitions=None)\n",
       "      For each key k in `self` or `other`, return a resulting RDD that\n",
       "      contains a tuple with the list of values for that key in `self` as\n",
       "      well as `other`.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; x = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 4)])\n",
       "      &gt;&gt;&gt; y = sc.parallelize([(&#34;a&#34;, 2)])\n",
       "      &gt;&gt;&gt; [(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]\n",
       "      [(&#39;a&#39;, ([1], [2])), (&#39;b&#39;, ([4], []))]\n",
       "  \n",
       "  collect(self)\n",
       "      Return a list that contains all of the elements in this RDD.\n",
       "      \n",
       "      Notes\n",
       "      -----\n",
       "      This method should only be used if the resulting array is expected\n",
       "      to be small, as all the data is loaded into the driver&#39;s memory.\n",
       "  \n",
       "  collectAsMap(self)\n",
       "      Return the key-value pairs in this RDD to the master as a dictionary.\n",
       "      \n",
       "      Notes\n",
       "      -----\n",
       "      This method should only be used if the resulting data is expected\n",
       "      to be small, as all the data is loaded into the driver&#39;s memory.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n",
       "      &gt;&gt;&gt; m[1]\n",
       "      2\n",
       "      &gt;&gt;&gt; m[3]\n",
       "      4\n",
       "  \n",
       "  collectWithJobGroup(self, groupId, description, interruptOnCancel=False)\n",
       "      When collect rdd, use this method to specify job group.\n",
       "      \n",
       "      .. versionadded:: 3.0.0\n",
       "      .. deprecated:: 3.1.0\n",
       "          Use :class:`pyspark.InheritableThread` with the pinned thread mode enabled.\n",
       "  \n",
       "  combineByKey(self, createCombiner, mergeValue, mergeCombiners, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f9589109670&gt;)\n",
       "      Generic function to combine the elements for each key using a custom\n",
       "      set of aggregation functions.\n",
       "      \n",
       "      Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a &#34;combined\n",
       "      type&#34; C.\n",
       "      \n",
       "      Users provide three functions:\n",
       "      \n",
       "          - `createCombiner`, which turns a V into a C (e.g., creates\n",
       "            a one-element list)\n",
       "          - `mergeValue`, to merge a V into a C (e.g., adds it to the end of\n",
       "            a list)\n",
       "          - `mergeCombiners`, to combine two C&#39;s into a single one (e.g., merges\n",
       "            the lists)\n",
       "      \n",
       "      To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n",
       "      modify and return their first argument instead of creating a new C.\n",
       "      \n",
       "      In addition, users can control the partitioning of the output RDD.\n",
       "      \n",
       "      Notes\n",
       "      -----\n",
       "      V and C can be different -- for example, one might group an RDD of type\n",
       "          (Int, Int) into an RDD of type (Int, List[Int]).\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; x = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 1), (&#34;a&#34;, 2)])\n",
       "      &gt;&gt;&gt; def to_list(a):\n",
       "      ...     return [a]\n",
       "      ...\n",
       "      &gt;&gt;&gt; def append(a, b):\n",
       "      ...     a.append(b)\n",
       "      ...     return a\n",
       "      ...\n",
       "      &gt;&gt;&gt; def extend(a, b):\n",
       "      ...     a.extend(b)\n",
       "      ...     return a\n",
       "      ...\n",
       "      &gt;&gt;&gt; sorted(x.combineByKey(to_list, append, extend).collect())\n",
       "      [(&#39;a&#39;, [1, 2]), (&#39;b&#39;, [1])]\n",
       "  \n",
       "  count(self)\n",
       "      Return the number of elements in this RDD.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; sc.parallelize([2, 3, 4]).count()\n",
       "      3\n",
       "  \n",
       "  countApprox(self, timeout, confidence=0.95)\n",
       "      Approximate version of count() that returns a potentially incomplete\n",
       "      result within a timeout, even if not all tasks have finished.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; rdd = sc.parallelize(range(1000), 10)\n",
       "      &gt;&gt;&gt; rdd.countApprox(1000, 1.0)\n",
       "      1000\n",
       "  \n",
       "  countApproxDistinct(self, relativeSD=0.05)\n",
       "      Return approximate number of distinct elements in the RDD.\n",
       "      \n",
       "      Parameters\n",
       "      ----------\n",
       "      relativeSD : float, optional\n",
       "          Relative accuracy. Smaller values create\n",
       "          counters that require more space.\n",
       "          It must be greater than 0.000017.\n",
       "      \n",
       "      Notes\n",
       "      -----\n",
       "      The algorithm used is based on streamlib&#39;s implementation of\n",
       "      `&#34;HyperLogLog in Practice: Algorithmic Engineering of a State\n",
       "      of The Art Cardinality Estimation Algorithm&#34;, available here\n",
       "      &lt;https://doi.org/10.1145/2452376.2452456&gt;`_.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n",
       "      &gt;&gt;&gt; 900 &lt; n &lt; 1100\n",
       "      True\n",
       "      &gt;&gt;&gt; n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n",
       "      &gt;&gt;&gt; 16 &lt; n &lt; 24\n",
       "      True\n",
       "  \n",
       "  countByKey(self)\n",
       "      Count the number of elements for each key, and return the result to the\n",
       "      master as a dictionary.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; rdd = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 1), (&#34;a&#34;, 1)])\n",
       "      &gt;&gt;&gt; sorted(rdd.countByKey().items())\n",
       "      [(&#39;a&#39;, 2), (&#39;b&#39;, 1)]\n",
       "  \n",
       "  countByValue(self)\n",
       "      Return the count of each unique value in this RDD as a dictionary of\n",
       "      (value, count) pairs.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n",
       "      [(1, 2), (2, 3)]\n",
       "  \n",
       "  distinct(self, numPartitions=None)\n",
       "      Return a new RDD containing the distinct elements in this RDD.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n",
       "      [1, 2, 3]\n",
       "  \n",
       "  filter(self, f)\n",
       "      Return a new RDD containing only the elements that satisfy a predicate.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
       "      &gt;&gt;&gt; rdd.filter(lambda x: x % 2 == 0).collect()\n",
       "      [2, 4]\n",
       "  \n",
       "  first(self)\n",
       "      Return the first element in this RDD.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; sc.parallelize([2, 3, 4]).first()\n",
       "      2\n",
       "      &gt;&gt;&gt; sc.parallelize([]).first()\n",
       "      Traceback (most recent call last):\n",
       "          ...\n",
       "      ValueError: RDD is empty\n",
       "  \n",
       "  flatMap(self, f, preservesPartitioning=False)\n",
       "      Return a new RDD by first applying a function to all elements of this\n",
       "      RDD, and then flattening the results.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; rdd = sc.parallelize([2, 3, 4])\n",
       "      &gt;&gt;&gt; sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n",
       "      [1, 1, 1, 2, 2, 3]\n",
       "      &gt;&gt;&gt; sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n",
       "      [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n",
       "  \n",
       "  flatMapValues(self, f)\n",
       "      Pass each value in the key-value pair RDD through a flatMap function\n",
       "      without changing the keys; this also retains the original RDD&#39;s\n",
       "      partitioning.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; x = sc.parallelize([(&#34;a&#34;, [&#34;x&#34;, &#34;y&#34;, &#34;z&#34;]), (&#34;b&#34;, [&#34;p&#34;, &#34;r&#34;])])\n",
       "      &gt;&gt;&gt; def f(x): return x\n",
       "      &gt;&gt;&gt; x.flatMapValues(f).collect()\n",
       "      [(&#39;a&#39;, &#39;x&#39;), (&#39;a&#39;, &#39;y&#39;), (&#39;a&#39;, &#39;z&#39;), (&#39;b&#39;, &#39;p&#39;), (&#39;b&#39;, &#39;r&#39;)]\n",
       "  \n",
       "  fold(self, zeroValue, op)\n",
       "      Aggregate the elements of each partition, and then the results for all\n",
       "      the partitions, using a given associative function and a neutral &#34;zero value.&#34;\n",
       "      \n",
       "      The function ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n",
       "      as its result value to avoid object allocation; however, it should not\n",
       "      modify ``t2``.\n",
       "      \n",
       "      This behaves somewhat differently from fold operations implemented\n",
       "      for non-distributed collections in functional languages like Scala.\n",
       "      This fold operation may be applied to partitions individually, and then\n",
       "      fold those results into the final result, rather than apply the fold\n",
       "      to each element sequentially in some defined ordering. For functions\n",
       "      that are not commutative, the result may differ from that of a fold\n",
       "      applied to a non-distributed collection.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; from operator import add\n",
       "      &gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n",
       "      15\n",
       "  \n",
       "  foldByKey(self, zeroValue, func, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f9589109670&gt;)\n",
       "      Merge the values for each key using an associative function &#34;func&#34;\n",
       "      and a neutral &#34;zeroValue&#34; which may be added to the result an\n",
       "      arbitrary number of times, and must not change the result\n",
       "      (e.g., 0 for addition, or 1 for multiplication.).\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; rdd = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 1), (&#34;a&#34;, 1)])\n",
       "      &gt;&gt;&gt; from operator import add\n",
       "      &gt;&gt;&gt; sorted(rdd.foldByKey(0, add).collect())\n",
       "      [(&#39;a&#39;, 2), (&#39;b&#39;, 1)]\n",
       "  \n",
       "  foreach(self, f)\n",
       "      Applies a function to all elements of this RDD.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; def f(x): print(x)\n",
       "      &gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n",
       "  \n",
       "  foreachPartition(self, f)\n",
       "      Applies a function to each partition of this RDD.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; def f(iterator):\n",
       "      ...     for x in iterator:\n",
       "      ...          print(x)\n",
       "      &gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n",
       "  \n",
       "  fullOuterJoin(self, other, numPartitions=None)\n",
       "      Perform a right outer join of `self` and `other`.\n",
       "      \n",
       "      For each element (k, v) in `self`, the resulting RDD will either\n",
       "      contain all pairs (k, (v, w)) for w in `other`, or the pair\n",
       "      (k, (v, None)) if no elements in `other` have key k.\n",
       "      \n",
       "      Similarly, for each element (k, w) in `other`, the resulting RDD will\n",
       "      either contain all pairs (k, (v, w)) for v in `self`, or the pair\n",
       "      (k, (None, w)) if no elements in `self` have key k.\n",
       "      \n",
       "      Hash-partitions the resulting RDD into the given number of partitions.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; x = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 4)])\n",
       "      &gt;&gt;&gt; y = sc.parallelize([(&#34;a&#34;, 2), (&#34;c&#34;, 8)])\n",
       "      &gt;&gt;&gt; sorted(x.fullOuterJoin(y).collect())\n",
       "      [(&#39;a&#39;, (1, 2)), (&#39;b&#39;, (4, None)), (&#39;c&#39;, (None, 8))]\n",
       "  \n",
       "  getCheckpointFile(self)\n",
       "      Gets the name of the file to which this RDD was checkpointed\n",
       "      \n",
       "      Not defined if RDD is checkpointed locally.\n",
       "  \n",
       "  getNumPartitions(self)\n",
       "      Returns the number of partitions in RDD\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
       "      &gt;&gt;&gt; rdd.getNumPartitions()\n",
       "      2\n",
       "  \n",
       "  getResourceProfile(self)\n",
       "      Get the :class:`pyspark.resource.ResourceProfile` specified with this RDD or None\n",
       "      if it wasn&#39;t specified.\n",
       "      \n",
       "      .. versionadded:: 3.1.0\n",
       "      \n",
       "      Returns\n",
       "      -------\n",
       "      :py:class:`pyspark.resource.ResourceProfile`\n",
       "          The the user specified profile or None if none were specified\n",
       "      \n",
       "      Notes\n",
       "      -----\n",
       "      This API is experimental\n",
       "  \n",
       "  getStorageLevel(self)\n",
       "      Get the RDD&#39;s current storage level.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; rdd1 = sc.parallelize([1,2])\n",
       "      &gt;&gt;&gt; rdd1.getStorageLevel()\n",
       "      StorageLevel(False, False, False, False, 1)\n",
       "      &gt;&gt;&gt; print(rdd1.getStorageLevel())\n",
       "      Serialized 1x Replicated\n",
       "  \n",
       "  glom(self)\n",
       "      Return an RDD created by coalescing all elements within each partition\n",
       "      into a list.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
       "      &gt;&gt;&gt; sorted(rdd.glom().collect())\n",
       "      [[1, 2], [3, 4]]\n",
       "  \n",
       "  groupBy(self, f, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f9589109670&gt;)\n",
       "      Return an RDD of grouped items.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
       "      &gt;&gt;&gt; result = rdd.groupBy(lambda x: x % 2).collect()\n",
       "      &gt;&gt;&gt; sorted([(x, sorted(y)) for (x, y) in result])\n",
       "      [(0, [2, 8]), (1, [1, 1, 3, 5])]\n",
       "  \n",
       "  groupByKey(self, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f9589109670&gt;)\n",
       "      Group the values for each key in the RDD into a single sequence.\n",
       "      Hash-partitions the resulting RDD with numPartitions partitions.\n",
       "      \n",
       "      Notes\n",
       "      -----\n",
       "      If you are grouping in order to perform an aggregation (such as a\n",
       "      sum or average) over each key, using reduceByKey or aggregateByKey will\n",
       "      provide much better performance.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; rdd = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 1), (&#34;a&#34;, 1)])\n",
       "      &gt;&gt;&gt; sorted(rdd.groupByKey().mapValues(len).collect())\n",
       "      [(&#39;a&#39;, 2), (&#39;b&#39;, 1)]\n",
       "      &gt;&gt;&gt; sorted(rdd.groupByKey().mapValues(list).collect())\n",
       "      [(&#39;a&#39;, [1, 1]), (&#39;b&#39;, [1])]\n",
       "  \n",
       "  groupWith(self, other, *others)\n",
       "      Alias for cogroup but with support for multiple RDDs.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; w = sc.parallelize([(&#34;a&#34;, 5), (&#34;b&#34;, 6)])\n",
       "      &gt;&gt;&gt; x = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 4)])\n",
       "      &gt;&gt;&gt; y = sc.parallelize([(&#34;a&#34;, 2)])\n",
       "      &gt;&gt;&gt; z = sc.parallelize([(&#34;b&#34;, 42)])\n",
       "      &gt;&gt;&gt; [(x, tuple(map(list, y))) for x, y in sorted(list(w.groupWith(x, y, z).collect()))]\n",
       "      [(&#39;a&#39;, ([5], [1], [2], [])), (&#39;b&#39;, ([6], [4], [], [42]))]\n",
       "  \n",
       "  histogram(self, buckets)\n",
       "      Compute a histogram using the provided buckets. The buckets\n",
       "      are all open to the right except for the last which is closed.\n",
       "      e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n",
       "      which means 1&lt;=x&lt;10, 10&lt;=x&lt;20, 20&lt;=x&lt;=50. And on the input of 1\n",
       "      and 50 we would have a histogram of 1,0,1.\n",
       "      \n",
       "      If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n",
       "      this can be switched from an O(log n) insertion to O(1) per\n",
       "      element (where n is the number of buckets).\n",
       "      \n",
       "      Buckets must be sorted, not contain any duplicates, and have\n",
       "      at least two elements.\n",
       "      \n",
       "      If `buckets` is a number, it will generate buckets which are\n",
       "      evenly spaced between the minimum and maximum of the RDD. For\n",
       "      example, if the min value is 0 and the max is 100, given `buckets`\n",
       "      as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\n",
       "      be at least 1. An exception is raised if the RDD contains infinity.\n",
       "      If the elements in the RDD do not vary (max == min), a single bucket\n",
       "      will be used.\n",
       "      \n",
       "      The return value is a tuple of buckets and histogram.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; rdd = sc.parallelize(range(51))\n",
       "      &gt;&gt;&gt; rdd.histogram(2)\n",
       "      ([0, 25, 50], [25, 26])\n",
       "      &gt;&gt;&gt; rdd.histogram([0, 5, 25, 50])\n",
       "      ([0, 5, 25, 50], [5, 20, 26])\n",
       "      &gt;&gt;&gt; rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n",
       "      ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n",
       "      &gt;&gt;&gt; rdd = sc.parallelize([&#34;ab&#34;, &#34;ac&#34;, &#34;b&#34;, &#34;bd&#34;, &#34;ef&#34;])\n",
       "      &gt;&gt;&gt; rdd.histogram((&#34;a&#34;, &#34;b&#34;, &#34;c&#34;))\n",
       "      ((&#39;a&#39;, &#39;b&#39;, &#39;c&#39;), [2, 2])\n",
       "  \n",
       "  id(self)\n",
       "      A unique ID for this RDD (within its SparkContext).\n",
       "  \n",
       "  intersection(self, other)\n",
       "      Return the intersection of this RDD and another one. The output will\n",
       "      not contain any duplicate elements, even if the input RDDs did.\n",
       "      \n",
       "      Notes\n",
       "      -----\n",
       "      This method performs a shuffle internally.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
       "      &gt;&gt;&gt; rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
       "      &gt;&gt;&gt; rdd1.intersection(rdd2).collect()\n",
       "      [1, 2, 3]\n",
       "  \n",
       "  isCheckpointed(self)\n",
       "      Return whether this RDD is checkpointed and materialized, either reliably or locally.\n",
       "  \n",
       "  isEmpty(self)\n",
       "      Returns true if and only if the RDD contains no elements at all.\n",
       "      \n",
       "      Notes\n",
       "      -----\n",
       "      An RDD may be empty even when it has at least 1 partition.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; sc.parallelize([]).isEmpty()\n",
       "      True\n",
       "      &gt;&gt;&gt; sc.parallelize([1]).isEmpty()\n",
       "      False\n",
       "  \n",
       "  isLocallyCheckpointed(self)\n",
       "      Return whether this RDD is marked for local checkpointing.\n",
       "      \n",
       "      Exposed for testing.\n",
       "  \n",
       "  join(self, other, numPartitions=None)\n",
       "      Return an RDD containing all pairs of elements with matching keys in\n",
       "      `self` and `other`.\n",
       "      \n",
       "      Each pair of elements will be returned as a (k, (v1, v2)) tuple, where\n",
       "      (k, v1) is in `self` and (k, v2) is in `other`.\n",
       "      \n",
       "      Performs a hash join across the cluster.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; x = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 4)])\n",
       "      &gt;&gt;&gt; y = sc.parallelize([(&#34;a&#34;, 2), (&#34;a&#34;, 3)])\n",
       "      &gt;&gt;&gt; sorted(x.join(y).collect())\n",
       "      [(&#39;a&#39;, (1, 2)), (&#39;a&#39;, (1, 3))]\n",
       "  \n",
       "  keyBy(self, f)\n",
       "      Creates tuples of the elements in this RDD by applying `f`.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)\n",
       "      &gt;&gt;&gt; y = sc.parallelize(zip(range(0,5), range(0,5)))\n",
       "      &gt;&gt;&gt; [(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]\n",
       "      [(0, [[0], [0]]), (1, [[1], [1]]), (2, [[], [2]]), (3, [[], [3]]), (4, [[2], [4]])]\n",
       "  \n",
       "  keys(self)\n",
       "      Return an RDD with the keys of each tuple.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; m = sc.parallelize([(1, 2), (3, 4)]).keys()\n",
       "      &gt;&gt;&gt; m.collect()\n",
       "      [1, 3]\n",
       "  \n",
       "  leftOuterJoin(self, other, numPartitions=None)\n",
       "      Perform a left outer join of `self` and `other`.\n",
       "      \n",
       "      For each element (k, v) in `self`, the resulting RDD will either\n",
       "      contain all pairs (k, (v, w)) for w in `other`, or the pair\n",
       "      (k, (v, None)) if no elements in `other` have key k.\n",
       "      \n",
       "      Hash-partitions the resulting RDD into the given number of partitions.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; x = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 4)])\n",
       "      &gt;&gt;&gt; y = sc.parallelize([(&#34;a&#34;, 2)])\n",
       "      &gt;&gt;&gt; sorted(x.leftOuterJoin(y).collect())\n",
       "      [(&#39;a&#39;, (1, 2)), (&#39;b&#39;, (4, None))]\n",
       "  \n",
       "  localCheckpoint(self)\n",
       "      Mark this RDD for local checkpointing using Spark&#39;s existing caching layer.\n",
       "      \n",
       "      This method is for users who wish to truncate RDD lineages while skipping the expensive\n",
       "      step of replicating the materialized data in a reliable distributed file system. This is\n",
       "      useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX).\n",
       "      \n",
       "      Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed\n",
       "      data is written to ephemeral local storage in the executors instead of to a reliable,\n",
       "      fault-tolerant storage. The effect is that if an executor fails during the computation,\n",
       "      the checkpointed data may no longer be accessible, causing an irrecoverable job failure.\n",
       "      \n",
       "      This is NOT safe to use with dynamic allocation, which removes executors along\n",
       "      with their cached blocks. If you must use both features, you are advised to set\n",
       "      `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value.\n",
       "      \n",
       "      The checkpoint directory set through :meth:`SparkContext.setCheckpointDir` is not used.\n",
       "  \n",
       "  lookup(self, key)\n",
       "      Return the list of values in the RDD for key `key`. This operation\n",
       "      is done efficiently if the RDD has a known partitioner by only\n",
       "      searching the partition that the key maps to.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; l = range(1000)\n",
       "      &gt;&gt;&gt; rdd = sc.parallelize(zip(l, l), 10)\n",
       "      &gt;&gt;&gt; rdd.lookup(42)  # slow\n",
       "      [42]\n",
       "      &gt;&gt;&gt; sorted = rdd.sortByKey()\n",
       "      &gt;&gt;&gt; sorted.lookup(42)  # fast\n",
       "      [42]\n",
       "      &gt;&gt;&gt; sorted.lookup(1024)\n",
       "      []\n",
       "      &gt;&gt;&gt; rdd2 = sc.parallelize([((&#39;a&#39;, &#39;b&#39;), &#39;c&#39;)]).groupByKey()\n",
       "      &gt;&gt;&gt; list(rdd2.lookup((&#39;a&#39;, &#39;b&#39;))[0])\n",
       "      [&#39;c&#39;]\n",
       "  \n",
       "  map(self, f, preservesPartitioning=False)\n",
       "      Return a new RDD by applying a function to each element of this RDD.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; rdd = sc.parallelize([&#34;b&#34;, &#34;a&#34;, &#34;c&#34;])\n",
       "      &gt;&gt;&gt; sorted(rdd.map(lambda x: (x, 1)).collect())\n",
       "      [(&#39;a&#39;, 1), (&#39;b&#39;, 1), (&#39;c&#39;, 1)]\n",
       "  \n",
       "  mapPartitions(self, f, preservesPartitioning=False)\n",
       "      Return a new RDD by applying a function to each partition of this RDD.\n",
       "\n",
       "*** WARNING: skipped 7146 bytes of output ***\n",
       "\n",
       "      \n",
       "       Can increase or decrease the level of parallelism in this RDD.\n",
       "       Internally, this uses a shuffle to redistribute data.\n",
       "       If you are decreasing the number of partitions in this RDD, consider\n",
       "       using `coalesce`, which can avoid performing a shuffle.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "       &gt;&gt;&gt; rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
       "       &gt;&gt;&gt; sorted(rdd.glom().collect())\n",
       "       [[1], [2, 3], [4, 5], [6, 7]]\n",
       "       &gt;&gt;&gt; len(rdd.repartition(2).glom().collect())\n",
       "       2\n",
       "       &gt;&gt;&gt; len(rdd.repartition(10).glom().collect())\n",
       "       10\n",
       "  \n",
       "  repartitionAndSortWithinPartitions(self, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f9589109670&gt;, ascending=True, keyfunc=&lt;function RDD.&lt;lambda&gt; at 0x7f958904cc10&gt;)\n",
       "      Repartition the RDD according to the given partitioner and, within each resulting partition,\n",
       "      sort records by their keys.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n",
       "      &gt;&gt;&gt; rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\n",
       "      &gt;&gt;&gt; rdd2.glom().collect()\n",
       "      [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n",
       "  \n",
       "  rightOuterJoin(self, other, numPartitions=None)\n",
       "      Perform a right outer join of `self` and `other`.\n",
       "      \n",
       "      For each element (k, w) in `other`, the resulting RDD will either\n",
       "      contain all pairs (k, (v, w)) for v in this, or the pair (k, (None, w))\n",
       "      if no elements in `self` have key k.\n",
       "      \n",
       "      Hash-partitions the resulting RDD into the given number of partitions.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; x = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 4)])\n",
       "      &gt;&gt;&gt; y = sc.parallelize([(&#34;a&#34;, 2)])\n",
       "      &gt;&gt;&gt; sorted(y.rightOuterJoin(x).collect())\n",
       "      [(&#39;a&#39;, (2, 1)), (&#39;b&#39;, (None, 4))]\n",
       "  \n",
       "  sample(self, withReplacement, fraction, seed=None)\n",
       "      Return a sampled subset of this RDD.\n",
       "      \n",
       "      Parameters\n",
       "      ----------\n",
       "      withReplacement : bool\n",
       "          can elements be sampled multiple times (replaced when sampled out)\n",
       "      fraction : float\n",
       "          expected size of the sample as a fraction of this RDD&#39;s size\n",
       "          without replacement: probability that each element is chosen; fraction must be [0, 1]\n",
       "          with replacement: expected number of times each element is chosen; fraction must be &gt;= 0\n",
       "      seed : int, optional\n",
       "          seed for the random number generator\n",
       "      \n",
       "      Notes\n",
       "      -----\n",
       "      This is not guaranteed to provide exactly the fraction specified of the total\n",
       "      count of the given :class:`DataFrame`.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; rdd = sc.parallelize(range(100), 4)\n",
       "      &gt;&gt;&gt; 6 &lt;= rdd.sample(False, 0.1, 81).count() &lt;= 14\n",
       "      True\n",
       "  \n",
       "  sampleByKey(self, withReplacement, fractions, seed=None)\n",
       "      Return a subset of this RDD sampled by key (via stratified sampling).\n",
       "      Create a sample of this RDD using variable sampling rates for\n",
       "      different keys as specified by fractions, a key to sampling rate map.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; fractions = {&#34;a&#34;: 0.2, &#34;b&#34;: 0.1}\n",
       "      &gt;&gt;&gt; rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n",
       "      &gt;&gt;&gt; sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n",
       "      &gt;&gt;&gt; 100 &lt; len(sample[&#34;a&#34;]) &lt; 300 and 50 &lt; len(sample[&#34;b&#34;]) &lt; 150\n",
       "      True\n",
       "      &gt;&gt;&gt; max(sample[&#34;a&#34;]) &lt;= 999 and min(sample[&#34;a&#34;]) &gt;= 0\n",
       "      True\n",
       "      &gt;&gt;&gt; max(sample[&#34;b&#34;]) &lt;= 999 and min(sample[&#34;b&#34;]) &gt;= 0\n",
       "      True\n",
       "  \n",
       "  sampleStdev(self)\n",
       "      Compute the sample standard deviation of this RDD&#39;s elements (which\n",
       "      corrects for bias in estimating the standard deviation by dividing by\n",
       "      N-1 instead of N).\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; sc.parallelize([1, 2, 3]).sampleStdev()\n",
       "      1.0\n",
       "  \n",
       "  sampleVariance(self)\n",
       "      Compute the sample variance of this RDD&#39;s elements (which corrects\n",
       "      for bias in estimating the variance by dividing by N-1 instead of N).\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; sc.parallelize([1, 2, 3]).sampleVariance()\n",
       "      1.0\n",
       "  \n",
       "  saveAsHadoopDataset(self, conf, keyConverter=None, valueConverter=None)\n",
       "      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
       "      system, using the old Hadoop OutputFormat API (mapred package). Keys/values are\n",
       "      converted for output using either user specified converters or, by default,\n",
       "      &#34;org.apache.spark.api.python.JavaToWritableConverter&#34;.\n",
       "      \n",
       "      Parameters\n",
       "      ----------\n",
       "      conf : dict\n",
       "          Hadoop job configuration\n",
       "      keyConverter : str, optional\n",
       "          fully qualified classname of key converter (None by default)\n",
       "      valueConverter : str, optional\n",
       "          fully qualified classname of value converter (None by default)\n",
       "  \n",
       "  saveAsHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None, compressionCodecClass=None)\n",
       "      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
       "      system, using the old Hadoop OutputFormat API (mapred package). Key and value types\n",
       "      will be inferred if not specified. Keys and values are converted for output using either\n",
       "      user specified converters or &#34;org.apache.spark.api.python.JavaToWritableConverter&#34;. The\n",
       "      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n",
       "      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
       "      \n",
       "      Parameters\n",
       "      ----------\n",
       "      path : str\n",
       "          path to Hadoop file\n",
       "      outputFormatClass : str\n",
       "          fully qualified classname of Hadoop OutputFormat\n",
       "          (e.g. &#34;org.apache.hadoop.mapred.SequenceFileOutputFormat&#34;)\n",
       "      keyClass : str, optional\n",
       "          fully qualified classname of key Writable class\n",
       "          (e.g. &#34;org.apache.hadoop.io.IntWritable&#34;, None by default)\n",
       "      valueClass : str, optional\n",
       "          fully qualified classname of value Writable class\n",
       "          (e.g. &#34;org.apache.hadoop.io.Text&#34;, None by default)\n",
       "      keyConverter : str, optional\n",
       "          fully qualified classname of key converter (None by default)\n",
       "      valueConverter : str, optional\n",
       "          fully qualified classname of value converter (None by default)\n",
       "      conf : dict, optional\n",
       "          (None by default)\n",
       "      compressionCodecClass : str\n",
       "          fully qualified classname of the compression codec class\n",
       "          i.e. &#34;org.apache.hadoop.io.compress.GzipCodec&#34; (None by default)\n",
       "  \n",
       "  saveAsNewAPIHadoopDataset(self, conf, keyConverter=None, valueConverter=None)\n",
       "      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
       "      system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n",
       "      converted for output using either user specified converters or, by default,\n",
       "      &#34;org.apache.spark.api.python.JavaToWritableConverter&#34;.\n",
       "      \n",
       "      Parameters\n",
       "      ----------\n",
       "      conf : dict\n",
       "          Hadoop job configuration\n",
       "      keyConverter : str, optional\n",
       "          fully qualified classname of key converter (None by default)\n",
       "      valueConverter : str, optional\n",
       "          fully qualified classname of value converter (None by default)\n",
       "  \n",
       "  saveAsNewAPIHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None)\n",
       "      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
       "      system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n",
       "      will be inferred if not specified. Keys and values are converted for output using either\n",
       "      user specified converters or &#34;org.apache.spark.api.python.JavaToWritableConverter&#34;. The\n",
       "      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n",
       "      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
       "      \n",
       "      path : str\n",
       "          path to Hadoop file\n",
       "      outputFormatClass : str\n",
       "          fully qualified classname of Hadoop OutputFormat\n",
       "          (e.g. &#34;org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat&#34;)\n",
       "      keyClass : str, optional\n",
       "          fully qualified classname of key Writable class\n",
       "           (e.g. &#34;org.apache.hadoop.io.IntWritable&#34;, None by default)\n",
       "      valueClass : str, optional\n",
       "          fully qualified classname of value Writable class\n",
       "          (e.g. &#34;org.apache.hadoop.io.Text&#34;, None by default)\n",
       "      keyConverter : str, optional\n",
       "          fully qualified classname of key converter (None by default)\n",
       "      valueConverter : str, optional\n",
       "          fully qualified classname of value converter (None by default)\n",
       "      conf : dict, optional\n",
       "          Hadoop job configuration (None by default)\n",
       "  \n",
       "  saveAsPickleFile(self, path, batchSize=10)\n",
       "      Save this RDD as a SequenceFile of serialized objects. The serializer\n",
       "      used is :class:`pyspark.serializers.PickleSerializer`, default batch size\n",
       "      is 10.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; from tempfile import NamedTemporaryFile\n",
       "      &gt;&gt;&gt; tmpFile = NamedTemporaryFile(delete=True)\n",
       "      &gt;&gt;&gt; tmpFile.close()\n",
       "      &gt;&gt;&gt; sc.parallelize([1, 2, &#39;spark&#39;, &#39;rdd&#39;]).saveAsPickleFile(tmpFile.name, 3)\n",
       "      &gt;&gt;&gt; sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())\n",
       "      [&#39;1&#39;, &#39;2&#39;, &#39;rdd&#39;, &#39;spark&#39;]\n",
       "  \n",
       "  saveAsSequenceFile(self, path, compressionCodecClass=None)\n",
       "      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
       "      system, using the &#34;org.apache.hadoop.io.Writable&#34; types that we convert from the\n",
       "      RDD&#39;s key and value types. The mechanism is as follows:\n",
       "      \n",
       "          1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.\n",
       "          2. Keys and values of this Java RDD are converted to Writables and written out.\n",
       "      \n",
       "      Parameters\n",
       "      ----------\n",
       "      path : str\n",
       "          path to sequence file\n",
       "      compressionCodecClass : str, optional\n",
       "          fully qualified classname of the compression codec class\n",
       "          i.e. &#34;org.apache.hadoop.io.compress.GzipCodec&#34; (None by default)\n",
       "  \n",
       "  saveAsTextFile(self, path, compressionCodecClass=None)\n",
       "      Save this RDD as a text file, using string representations of elements.\n",
       "      \n",
       "      Parameters\n",
       "      ----------\n",
       "      path : str\n",
       "          path to text file\n",
       "      compressionCodecClass : str, optional\n",
       "          fully qualified classname of the compression codec class\n",
       "          i.e. &#34;org.apache.hadoop.io.compress.GzipCodec&#34; (None by default)\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; from tempfile import NamedTemporaryFile\n",
       "      &gt;&gt;&gt; tempFile = NamedTemporaryFile(delete=True)\n",
       "      &gt;&gt;&gt; tempFile.close()\n",
       "      &gt;&gt;&gt; sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\n",
       "      &gt;&gt;&gt; from fileinput import input\n",
       "      &gt;&gt;&gt; from glob import glob\n",
       "      &gt;&gt;&gt; &#39;&#39;.join(sorted(input(glob(tempFile.name + &#34;/part-0000*&#34;))))\n",
       "      &#39;0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n&#39;\n",
       "      \n",
       "      Empty lines are tolerated when saving to text files.\n",
       "      \n",
       "      &gt;&gt;&gt; from tempfile import NamedTemporaryFile\n",
       "      &gt;&gt;&gt; tempFile2 = NamedTemporaryFile(delete=True)\n",
       "      &gt;&gt;&gt; tempFile2.close()\n",
       "      &gt;&gt;&gt; sc.parallelize([&#39;&#39;, &#39;foo&#39;, &#39;&#39;, &#39;bar&#39;, &#39;&#39;]).saveAsTextFile(tempFile2.name)\n",
       "      &gt;&gt;&gt; &#39;&#39;.join(sorted(input(glob(tempFile2.name + &#34;/part-0000*&#34;))))\n",
       "      &#39;\\n\\n\\nbar\\nfoo\\n&#39;\n",
       "      \n",
       "      Using compressionCodecClass\n",
       "      \n",
       "      &gt;&gt;&gt; from tempfile import NamedTemporaryFile\n",
       "      &gt;&gt;&gt; tempFile3 = NamedTemporaryFile(delete=True)\n",
       "      &gt;&gt;&gt; tempFile3.close()\n",
       "      &gt;&gt;&gt; codec = &#34;org.apache.hadoop.io.compress.GzipCodec&#34;\n",
       "      &gt;&gt;&gt; sc.parallelize([&#39;foo&#39;, &#39;bar&#39;]).saveAsTextFile(tempFile3.name, codec)\n",
       "      &gt;&gt;&gt; from fileinput import input, hook_compressed\n",
       "      &gt;&gt;&gt; result = sorted(input(glob(tempFile3.name + &#34;/part*.gz&#34;), openhook=hook_compressed))\n",
       "      &gt;&gt;&gt; b&#39;&#39;.join(result).decode(&#39;utf-8&#39;)\n",
       "      &#39;bar\\nfoo\\n&#39;\n",
       "  \n",
       "  setName(self, name)\n",
       "      Assign a name to this RDD.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; rdd1 = sc.parallelize([1, 2])\n",
       "      &gt;&gt;&gt; rdd1.setName(&#39;RDD1&#39;).name()\n",
       "      &#39;RDD1&#39;\n",
       "  \n",
       "  sortBy(self, keyfunc, ascending=True, numPartitions=None)\n",
       "      Sorts this RDD by the given keyfunc\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; tmp = [(&#39;a&#39;, 1), (&#39;b&#39;, 2), (&#39;1&#39;, 3), (&#39;d&#39;, 4), (&#39;2&#39;, 5)]\n",
       "      &gt;&gt;&gt; sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n",
       "      [(&#39;1&#39;, 3), (&#39;2&#39;, 5), (&#39;a&#39;, 1), (&#39;b&#39;, 2), (&#39;d&#39;, 4)]\n",
       "      &gt;&gt;&gt; sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n",
       "      [(&#39;a&#39;, 1), (&#39;b&#39;, 2), (&#39;1&#39;, 3), (&#39;d&#39;, 4), (&#39;2&#39;, 5)]\n",
       "  \n",
       "  sortByKey(self, ascending=True, numPartitions=None, keyfunc=&lt;function RDD.&lt;lambda&gt; at 0x7f958904cd30&gt;)\n",
       "      Sorts this RDD, which is assumed to consist of (key, value) pairs.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; tmp = [(&#39;a&#39;, 1), (&#39;b&#39;, 2), (&#39;1&#39;, 3), (&#39;d&#39;, 4), (&#39;2&#39;, 5)]\n",
       "      &gt;&gt;&gt; sc.parallelize(tmp).sortByKey().first()\n",
       "      (&#39;1&#39;, 3)\n",
       "      &gt;&gt;&gt; sc.parallelize(tmp).sortByKey(True, 1).collect()\n",
       "      [(&#39;1&#39;, 3), (&#39;2&#39;, 5), (&#39;a&#39;, 1), (&#39;b&#39;, 2), (&#39;d&#39;, 4)]\n",
       "      &gt;&gt;&gt; sc.parallelize(tmp).sortByKey(True, 2).collect()\n",
       "      [(&#39;1&#39;, 3), (&#39;2&#39;, 5), (&#39;a&#39;, 1), (&#39;b&#39;, 2), (&#39;d&#39;, 4)]\n",
       "      &gt;&gt;&gt; tmp2 = [(&#39;Mary&#39;, 1), (&#39;had&#39;, 2), (&#39;a&#39;, 3), (&#39;little&#39;, 4), (&#39;lamb&#39;, 5)]\n",
       "      &gt;&gt;&gt; tmp2.extend([(&#39;whose&#39;, 6), (&#39;fleece&#39;, 7), (&#39;was&#39;, 8), (&#39;white&#39;, 9)])\n",
       "      &gt;&gt;&gt; sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n",
       "      [(&#39;a&#39;, 3), (&#39;fleece&#39;, 7), (&#39;had&#39;, 2), (&#39;lamb&#39;, 5),...(&#39;white&#39;, 9), (&#39;whose&#39;, 6)]\n",
       "  \n",
       "  stats(self)\n",
       "      Return a :class:`StatCounter` object that captures the mean, variance\n",
       "      and count of the RDD&#39;s elements in one operation.\n",
       "  \n",
       "  stdev(self)\n",
       "      Compute the standard deviation of this RDD&#39;s elements.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; sc.parallelize([1, 2, 3]).stdev()\n",
       "      0.816...\n",
       "  \n",
       "  subtract(self, other, numPartitions=None)\n",
       "      Return each value in `self` that is not contained in `other`.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; x = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 4), (&#34;b&#34;, 5), (&#34;a&#34;, 3)])\n",
       "      &gt;&gt;&gt; y = sc.parallelize([(&#34;a&#34;, 3), (&#34;c&#34;, None)])\n",
       "      &gt;&gt;&gt; sorted(x.subtract(y).collect())\n",
       "      [(&#39;a&#39;, 1), (&#39;b&#39;, 4), (&#39;b&#39;, 5)]\n",
       "  \n",
       "  subtractByKey(self, other, numPartitions=None)\n",
       "      Return each (key, value) pair in `self` that has no pair with matching\n",
       "      key in `other`.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; x = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 4), (&#34;b&#34;, 5), (&#34;a&#34;, 2)])\n",
       "      &gt;&gt;&gt; y = sc.parallelize([(&#34;a&#34;, 3), (&#34;c&#34;, None)])\n",
       "      &gt;&gt;&gt; sorted(x.subtractByKey(y).collect())\n",
       "      [(&#39;b&#39;, 4), (&#39;b&#39;, 5)]\n",
       "  \n",
       "  sum(self)\n",
       "      Add up the elements in this RDD.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; sc.parallelize([1.0, 2.0, 3.0]).sum()\n",
       "      6.0\n",
       "  \n",
       "  sumApprox(self, timeout, confidence=0.95)\n",
       "      Approximate operation to return the sum within a timeout\n",
       "      or meet the confidence.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; rdd = sc.parallelize(range(1000), 10)\n",
       "      &gt;&gt;&gt; r = sum(range(1000))\n",
       "      &gt;&gt;&gt; abs(rdd.sumApprox(1000) - r) / r &lt; 0.05\n",
       "      True\n",
       "  \n",
       "  take(self, num)\n",
       "      Take the first num elements of the RDD.\n",
       "      \n",
       "      It works by first scanning one partition, and use the results from\n",
       "      that partition to estimate the number of additional partitions needed\n",
       "      to satisfy the limit.\n",
       "      \n",
       "      Translated from the Scala implementation in RDD#take().\n",
       "      \n",
       "      Notes\n",
       "      -----\n",
       "      This method should only be used if the resulting array is expected\n",
       "      to be small, as all the data is loaded into the driver&#39;s memory.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n",
       "      [2, 3]\n",
       "      &gt;&gt;&gt; sc.parallelize([2, 3, 4, 5, 6]).take(10)\n",
       "      [2, 3, 4, 5, 6]\n",
       "      &gt;&gt;&gt; sc.parallelize(range(100), 100).filter(lambda x: x &gt; 90).take(3)\n",
       "      [91, 92, 93]\n",
       "  \n",
       "  takeOrdered(self, num, key=None)\n",
       "      Get the N elements from an RDD ordered in ascending order or as\n",
       "      specified by the optional key function.\n",
       "      \n",
       "      Notes\n",
       "      -----\n",
       "      This method should only be used if the resulting array is expected\n",
       "      to be small, as all the data is loaded into the driver&#39;s memory.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n",
       "      [1, 2, 3, 4, 5, 6]\n",
       "      &gt;&gt;&gt; sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n",
       "      [10, 9, 7, 6, 5, 4]\n",
       "  \n",
       "  takeSample(self, withReplacement, num, seed=None)\n",
       "      Return a fixed-size sampled subset of this RDD.\n",
       "      \n",
       "      Notes\n",
       "      -----\n",
       "      This method should only be used if the resulting array is expected\n",
       "      to be small, as all the data is loaded into the driver&#39;s memory.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; rdd = sc.parallelize(range(0, 10))\n",
       "      &gt;&gt;&gt; len(rdd.takeSample(True, 20, 1))\n",
       "      20\n",
       "      &gt;&gt;&gt; len(rdd.takeSample(False, 5, 2))\n",
       "      5\n",
       "      &gt;&gt;&gt; len(rdd.takeSample(False, 15, 3))\n",
       "      10\n",
       "  \n",
       "  toDF(self, schema=None, sampleRatio=None)\n",
       "      Converts current :class:`RDD` into a :class:`DataFrame`\n",
       "      \n",
       "      This is a shorthand for ``spark.createDataFrame(rdd, schema, sampleRatio)``\n",
       "      \n",
       "      Parameters\n",
       "      ----------\n",
       "      schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
       "          a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
       "          column names, default is None.  The data type string format equals to\n",
       "          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
       "          omit the ``struct&lt;&gt;`` and atomic types use ``typeName()`` as their format, e.g. use\n",
       "          ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\n",
       "          We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\n",
       "      sampleRatio : float, optional\n",
       "          the sample ratio of rows used for inferring\n",
       "      \n",
       "      Returns\n",
       "      -------\n",
       "      :class:`DataFrame`\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; rdd.toDF().collect()\n",
       "      [Row(name=&#39;Alice&#39;, age=1)]\n",
       "  \n",
       "  toDebugString(self)\n",
       "      A description of this RDD and its recursive dependencies for debugging.\n",
       "  \n",
       "  toLocalIterator(self, prefetchPartitions=False)\n",
       "      Return an iterator that contains all of the elements in this RDD.\n",
       "      The iterator will consume as much memory as the largest partition in this RDD.\n",
       "      With prefetch it may consume up to the memory of the 2 largest partitions.\n",
       "      \n",
       "      Parameters\n",
       "      ----------\n",
       "      prefetchPartitions : bool, optional\n",
       "          If Spark should pre-fetch the next partition\n",
       "          before it is needed.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; rdd = sc.parallelize(range(10))\n",
       "      &gt;&gt;&gt; [x for x in rdd.toLocalIterator()]\n",
       "      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
       "  \n",
       "  top(self, num, key=None)\n",
       "      Get the top N elements from an RDD.\n",
       "      \n",
       "      Notes\n",
       "      -----\n",
       "      This method should only be used if the resulting array is expected\n",
       "      to be small, as all the data is loaded into the driver&#39;s memory.\n",
       "      \n",
       "      It returns the list sorted in descending order.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; sc.parallelize([10, 4, 2, 12, 3]).top(1)\n",
       "      [12]\n",
       "      &gt;&gt;&gt; sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n",
       "      [6, 5]\n",
       "      &gt;&gt;&gt; sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n",
       "      [4, 3, 2]\n",
       "  \n",
       "  treeAggregate(self, zeroValue, seqOp, combOp, depth=2)\n",
       "      Aggregates the elements of this RDD in a multi-level tree\n",
       "      pattern.\n",
       "      \n",
       "      depth : int, optional\n",
       "          suggested depth of the tree (default: 2)\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; add = lambda x, y: x + y\n",
       "      &gt;&gt;&gt; rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
       "      &gt;&gt;&gt; rdd.treeAggregate(0, add, add)\n",
       "      -5\n",
       "      &gt;&gt;&gt; rdd.treeAggregate(0, add, add, 1)\n",
       "      -5\n",
       "      &gt;&gt;&gt; rdd.treeAggregate(0, add, add, 2)\n",
       "      -5\n",
       "      &gt;&gt;&gt; rdd.treeAggregate(0, add, add, 5)\n",
       "      -5\n",
       "      &gt;&gt;&gt; rdd.treeAggregate(0, add, add, 10)\n",
       "      -5\n",
       "  \n",
       "  treeReduce(self, f, depth=2)\n",
       "      Reduces the elements of this RDD in a multi-level tree pattern.\n",
       "      \n",
       "      Parameters\n",
       "      ----------\n",
       "      f : function\n",
       "      depth : int, optional\n",
       "          suggested depth of the tree (default: 2)\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; add = lambda x, y: x + y\n",
       "      &gt;&gt;&gt; rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
       "      &gt;&gt;&gt; rdd.treeReduce(add)\n",
       "      -5\n",
       "      &gt;&gt;&gt; rdd.treeReduce(add, 1)\n",
       "      -5\n",
       "      &gt;&gt;&gt; rdd.treeReduce(add, 2)\n",
       "      -5\n",
       "      &gt;&gt;&gt; rdd.treeReduce(add, 5)\n",
       "      -5\n",
       "      &gt;&gt;&gt; rdd.treeReduce(add, 10)\n",
       "      -5\n",
       "  \n",
       "  union(self, other)\n",
       "      Return the union of this RDD and another one.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; rdd = sc.parallelize([1, 1, 2, 3])\n",
       "      &gt;&gt;&gt; rdd.union(rdd).collect()\n",
       "      [1, 1, 2, 3, 1, 1, 2, 3]\n",
       "  \n",
       "  unpersist(self, blocking=False)\n",
       "      Mark the RDD as non-persistent, and remove all blocks for it from\n",
       "      memory and disk.\n",
       "      \n",
       "      .. versionchanged:: 3.0.0\n",
       "         Added optional argument `blocking` to specify whether to block until all\n",
       "         blocks are deleted.\n",
       "  \n",
       "  values(self)\n",
       "      Return an RDD with the values of each tuple.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; m = sc.parallelize([(1, 2), (3, 4)]).values()\n",
       "      &gt;&gt;&gt; m.collect()\n",
       "      [2, 4]\n",
       "  \n",
       "  variance(self)\n",
       "      Compute the variance of this RDD&#39;s elements.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; sc.parallelize([1, 2, 3]).variance()\n",
       "      0.666...\n",
       "  \n",
       "  withResources(self, profile)\n",
       "      Specify a :class:`pyspark.resource.ResourceProfile` to use when calculating this RDD.\n",
       "      This is only supported on certain cluster managers and currently requires dynamic\n",
       "      allocation to be enabled. It will result in new executors with the resources specified\n",
       "      being acquired to calculate the RDD.\n",
       "      \n",
       "      .. versionadded:: 3.1.0\n",
       "      \n",
       "      Notes\n",
       "      -----\n",
       "      This API is experimental\n",
       "  \n",
       "  zip(self, other)\n",
       "      Zips this RDD with another one, returning key-value pairs with the\n",
       "      first element in each RDD second element in each RDD, etc. Assumes\n",
       "      that the two RDDs have the same number of partitions and the same\n",
       "      number of elements in each partition (e.g. one was made through\n",
       "      a map on the other).\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; x = sc.parallelize(range(0,5))\n",
       "      &gt;&gt;&gt; y = sc.parallelize(range(1000, 1005))\n",
       "      &gt;&gt;&gt; x.zip(y).collect()\n",
       "      [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n",
       "  \n",
       "  zipWithIndex(self)\n",
       "      Zips this RDD with its element indices.\n",
       "      \n",
       "      The ordering is first based on the partition index and then the\n",
       "      ordering of items within each partition. So the first item in\n",
       "      the first partition gets index 0, and the last item in the last\n",
       "      partition receives the largest index.\n",
       "      \n",
       "      This method needs to trigger a spark job when this RDD contains\n",
       "      more than one partitions.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; sc.parallelize([&#34;a&#34;, &#34;b&#34;, &#34;c&#34;, &#34;d&#34;], 3).zipWithIndex().collect()\n",
       "      [(&#39;a&#39;, 0), (&#39;b&#39;, 1), (&#39;c&#39;, 2), (&#39;d&#39;, 3)]\n",
       "  \n",
       "  zipWithUniqueId(self)\n",
       "      Zips this RDD with generated unique Long ids.\n",
       "      \n",
       "      Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n",
       "      n is the number of partitions. So there may exist gaps, but this\n",
       "      method won&#39;t trigger a spark job, which is different from\n",
       "      :meth:`zipWithIndex`.\n",
       "      \n",
       "      Examples\n",
       "      --------\n",
       "      &gt;&gt;&gt; sc.parallelize([&#34;a&#34;, &#34;b&#34;, &#34;c&#34;, &#34;d&#34;, &#34;e&#34;], 3).zipWithUniqueId().collect()\n",
       "      [(&#39;a&#39;, 0), (&#39;b&#39;, 1), (&#39;c&#39;, 4), (&#39;d&#39;, 2), (&#39;e&#39;, 5)]\n",
       "  \n",
       "  ----------------------------------------------------------------------\n",
       "  Readonly properties defined here:\n",
       "  \n",
       "  context\n",
       "      The :class:`SparkContext` that this RDD was created on.\n",
       "  \n",
       "  ----------------------------------------------------------------------\n",
       "  Data descriptors defined here:\n",
       "  \n",
       "  __dict__\n",
       "      dictionary for instance variables (if defined)\n",
       "  \n",
       "  __weakref__\n",
       "      list of weak references to the object (if defined)\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Help on RDD in module pyspark.rdd object:\n\nclass RDD(builtins.object)\n |  RDD(jrdd, ctx, jrdd_deserializer=AutoBatchedSerializer(PickleSerializer()))\n |  \n |  A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n |  Represents an immutable, partitioned collection of elements that can be\n |  operated on in parallel.\n |  \n |  Methods defined here:\n |  \n |  __add__(self, other)\n |      Return the union of this RDD and another one.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; rdd = sc.parallelize([1, 1, 2, 3])\n |      &gt;&gt;&gt; (rdd + rdd).collect()\n |      [1, 1, 2, 3, 1, 1, 2, 3]\n |  \n |  __getnewargs__(self)\n |  \n |  __init__(self, jrdd, ctx, jrdd_deserializer=AutoBatchedSerializer(PickleSerializer()))\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __repr__(self)\n |      Return repr(self).\n |  \n |  aggregate(self, zeroValue, seqOp, combOp)\n |      Aggregate the elements of each partition, and then the results for all\n |      the partitions, using a given combine functions and a neutral &#34;zero\n |      value.&#34;\n |      \n |      The functions ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n |      as its result value to avoid object allocation; however, it should not\n |      modify ``t2``.\n |      \n |      The first function (seqOp) can return a different result type, U, than\n |      the type of this RDD. Thus, we need one operation for merging a T into\n |      an U and one operation for merging two U\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n |      &gt;&gt;&gt; combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n |      &gt;&gt;&gt; sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n |      (10, 4)\n |      &gt;&gt;&gt; sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n |      (0, 0)\n |  \n |  aggregateByKey(self, zeroValue, seqFunc, combFunc, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f9589109670&gt;)\n |      Aggregate the values of each key, using given combine functions and a neutral\n |      &#34;zero value&#34;. This function can return a different result type, U, than the type\n |      of the values in this RDD, V. Thus, we need one operation for merging a V into\n |      a U and one operation for merging two U&#39;s, The former operation is used for merging\n |      values within a partition, and the latter is used for merging values between\n |      partitions. To avoid memory allocation, both of these functions are\n |      allowed to modify and return their first argument instead of creating a new U.\n |  \n |  barrier(self)\n |      Marks the current stage as a barrier stage, where Spark must launch all tasks together.\n |      In case of a task failure, instead of only restarting the failed task, Spark will abort the\n |      entire stage and relaunch all tasks for this stage.\n |      The barrier execution mode feature is experimental and it only handles limited scenarios.\n |      Please read the linked SPIP and design docs to understand the limitations and future plans.\n |      \n |      .. versionadded:: 2.4.0\n |      \n |      Returns\n |      -------\n |      :class:`RDDBarrier`\n |          instance that provides actions within a barrier stage.\n |      \n |      See Also\n |      --------\n |      pyspark.BarrierTaskContext\n |      \n |      Notes\n |      -----\n |      For additional information see\n |      \n |      - `SPIP: Barrier Execution Mode &lt;http://jira.apache.org/jira/browse/SPARK-24374&gt;`_\n |      - `Design Doc &lt;https://jira.apache.org/jira/browse/SPARK-24582&gt;`_\n |      \n |      This API is experimental\n |  \n |  cache(self)\n |      Persist this RDD with the default storage level (`MEMORY_ONLY`).\n |  \n |  cartesian(self, other)\n |      Return the Cartesian product of this RDD and another one, that is, the\n |      RDD of all pairs of elements ``(a, b)`` where ``a`` is in `self` and\n |      ``b`` is in `other`.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; rdd = sc.parallelize([1, 2])\n |      &gt;&gt;&gt; sorted(rdd.cartesian(rdd).collect())\n |      [(1, 1), (1, 2), (2, 1), (2, 2)]\n |  \n |  checkpoint(self)\n |      Mark this RDD for checkpointing. It will be saved to a file inside the\n |      checkpoint directory set with :meth:`SparkContext.setCheckpointDir` and\n |      all references to its parent RDDs will be removed. This function must\n |      be called before any job has been executed on this RDD. It is strongly\n |      recommended that this RDD is persisted in memory, otherwise saving it\n |      on a file will require recomputation.\n |  \n |  coalesce(self, numPartitions, shuffle=False)\n |      Return a new RDD that is reduced into `numPartitions` partitions.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n |      [[1], [2, 3], [4, 5]]\n |      &gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n |      [[1, 2, 3, 4, 5]]\n |  \n |  cogroup(self, other, numPartitions=None)\n |      For each key k in `self` or `other`, return a resulting RDD that\n |      contains a tuple with the list of values for that key in `self` as\n |      well as `other`.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 4)])\n |      &gt;&gt;&gt; y = sc.parallelize([(&#34;a&#34;, 2)])\n |      &gt;&gt;&gt; [(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]\n |      [(&#39;a&#39;, ([1], [2])), (&#39;b&#39;, ([4], []))]\n |  \n |  collect(self)\n |      Return a list that contains all of the elements in this RDD.\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver&#39;s memory.\n |  \n |  collectAsMap(self)\n |      Return the key-value pairs in this RDD to the master as a dictionary.\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting data is expected\n |      to be small, as all the data is loaded into the driver&#39;s memory.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n |      &gt;&gt;&gt; m[1]\n |      2\n |      &gt;&gt;&gt; m[3]\n |      4\n |  \n |  collectWithJobGroup(self, groupId, description, interruptOnCancel=False)\n |      When collect rdd, use this method to specify job group.\n |      \n |      .. versionadded:: 3.0.0\n |      .. deprecated:: 3.1.0\n |          Use :class:`pyspark.InheritableThread` with the pinned thread mode enabled.\n |  \n |  combineByKey(self, createCombiner, mergeValue, mergeCombiners, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f9589109670&gt;)\n |      Generic function to combine the elements for each key using a custom\n |      set of aggregation functions.\n |      \n |      Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a &#34;combined\n |      type&#34; C.\n |      \n |      Users provide three functions:\n |      \n |          - `createCombiner`, which turns a V into a C (e.g., creates\n |            a one-element list)\n |          - `mergeValue`, to merge a V into a C (e.g., adds it to the end of\n |            a list)\n |          - `mergeCombiners`, to combine two C&#39;s into a single one (e.g., merges\n |            the lists)\n |      \n |      To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n |      modify and return their first argument instead of creating a new C.\n |      \n |      In addition, users can control the partitioning of the output RDD.\n |      \n |      Notes\n |      -----\n |      V and C can be different -- for example, one might group an RDD of type\n |          (Int, Int) into an RDD of type (Int, List[Int]).\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 1), (&#34;a&#34;, 2)])\n |      &gt;&gt;&gt; def to_list(a):\n |      ...     return [a]\n |      ...\n |      &gt;&gt;&gt; def append(a, b):\n |      ...     a.append(b)\n |      ...     return a\n |      ...\n |      &gt;&gt;&gt; def extend(a, b):\n |      ...     a.extend(b)\n |      ...     return a\n |      ...\n |      &gt;&gt;&gt; sorted(x.combineByKey(to_list, append, extend).collect())\n |      [(&#39;a&#39;, [1, 2]), (&#39;b&#39;, [1])]\n |  \n |  count(self)\n |      Return the number of elements in this RDD.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; sc.parallelize([2, 3, 4]).count()\n |      3\n |  \n |  countApprox(self, timeout, confidence=0.95)\n |      Approximate version of count() that returns a potentially incomplete\n |      result within a timeout, even if not all tasks have finished.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; rdd = sc.parallelize(range(1000), 10)\n |      &gt;&gt;&gt; rdd.countApprox(1000, 1.0)\n |      1000\n |  \n |  countApproxDistinct(self, relativeSD=0.05)\n |      Return approximate number of distinct elements in the RDD.\n |      \n |      Parameters\n |      ----------\n |      relativeSD : float, optional\n |          Relative accuracy. Smaller values create\n |          counters that require more space.\n |          It must be greater than 0.000017.\n |      \n |      Notes\n |      -----\n |      The algorithm used is based on streamlib&#39;s implementation of\n |      `&#34;HyperLogLog in Practice: Algorithmic Engineering of a State\n |      of The Art Cardinality Estimation Algorithm&#34;, available here\n |      &lt;https://doi.org/10.1145/2452376.2452456&gt;`_.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n |      &gt;&gt;&gt; 900 &lt; n &lt; 1100\n |      True\n |      &gt;&gt;&gt; n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n |      &gt;&gt;&gt; 16 &lt; n &lt; 24\n |      True\n |  \n |  countByKey(self)\n |      Count the number of elements for each key, and return the result to the\n |      master as a dictionary.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; rdd = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 1), (&#34;a&#34;, 1)])\n |      &gt;&gt;&gt; sorted(rdd.countByKey().items())\n |      [(&#39;a&#39;, 2), (&#39;b&#39;, 1)]\n |  \n |  countByValue(self)\n |      Return the count of each unique value in this RDD as a dictionary of\n |      (value, count) pairs.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n |      [(1, 2), (2, 3)]\n |  \n |  distinct(self, numPartitions=None)\n |      Return a new RDD containing the distinct elements in this RDD.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n |      [1, 2, 3]\n |  \n |  filter(self, f)\n |      Return a new RDD containing only the elements that satisfy a predicate.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4, 5])\n |      &gt;&gt;&gt; rdd.filter(lambda x: x % 2 == 0).collect()\n |      [2, 4]\n |  \n |  first(self)\n |      Return the first element in this RDD.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; sc.parallelize([2, 3, 4]).first()\n |      2\n |      &gt;&gt;&gt; sc.parallelize([]).first()\n |      Traceback (most recent call last):\n |          ...\n |      ValueError: RDD is empty\n |  \n |  flatMap(self, f, preservesPartitioning=False)\n |      Return a new RDD by first applying a function to all elements of this\n |      RDD, and then flattening the results.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; rdd = sc.parallelize([2, 3, 4])\n |      &gt;&gt;&gt; sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n |      [1, 1, 1, 2, 2, 3]\n |      &gt;&gt;&gt; sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n |      [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n |  \n |  flatMapValues(self, f)\n |      Pass each value in the key-value pair RDD through a flatMap function\n |      without changing the keys; this also retains the original RDD&#39;s\n |      partitioning.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = sc.parallelize([(&#34;a&#34;, [&#34;x&#34;, &#34;y&#34;, &#34;z&#34;]), (&#34;b&#34;, [&#34;p&#34;, &#34;r&#34;])])\n |      &gt;&gt;&gt; def f(x): return x\n |      &gt;&gt;&gt; x.flatMapValues(f).collect()\n |      [(&#39;a&#39;, &#39;x&#39;), (&#39;a&#39;, &#39;y&#39;), (&#39;a&#39;, &#39;z&#39;), (&#39;b&#39;, &#39;p&#39;), (&#39;b&#39;, &#39;r&#39;)]\n |  \n |  fold(self, zeroValue, op)\n |      Aggregate the elements of each partition, and then the results for all\n |      the partitions, using a given associative function and a neutral &#34;zero value.&#34;\n |      \n |      The function ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n |      as its result value to avoid object allocation; however, it should not\n |      modify ``t2``.\n |      \n |      This behaves somewhat differently from fold operations implemented\n |      for non-distributed collections in functional languages like Scala.\n |      This fold operation may be applied to partitions individually, and then\n |      fold those results into the final result, rather than apply the fold\n |      to each element sequentially in some defined ordering. For functions\n |      that are not commutative, the result may differ from that of a fold\n |      applied to a non-distributed collection.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; from operator import add\n |      &gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n |      15\n |  \n |  foldByKey(self, zeroValue, func, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f9589109670&gt;)\n |      Merge the values for each key using an associative function &#34;func&#34;\n |      and a neutral &#34;zeroValue&#34; which may be added to the result an\n |      arbitrary number of times, and must not change the result\n |      (e.g., 0 for addition, or 1 for multiplication.).\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; rdd = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 1), (&#34;a&#34;, 1)])\n |      &gt;&gt;&gt; from operator import add\n |      &gt;&gt;&gt; sorted(rdd.foldByKey(0, add).collect())\n |      [(&#39;a&#39;, 2), (&#39;b&#39;, 1)]\n |  \n |  foreach(self, f)\n |      Applies a function to all elements of this RDD.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; def f(x): print(x)\n |      &gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n |  \n |  foreachPartition(self, f)\n |      Applies a function to each partition of this RDD.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; def f(iterator):\n |      ...     for x in iterator:\n |      ...          print(x)\n |      &gt;&gt;&gt; sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n |  \n |  fullOuterJoin(self, other, numPartitions=None)\n |      Perform a right outer join of `self` and `other`.\n |      \n |      For each element (k, v) in `self`, the resulting RDD will either\n |      contain all pairs (k, (v, w)) for w in `other`, or the pair\n |      (k, (v, None)) if no elements in `other` have key k.\n |      \n |      Similarly, for each element (k, w) in `other`, the resulting RDD will\n |      either contain all pairs (k, (v, w)) for v in `self`, or the pair\n |      (k, (None, w)) if no elements in `self` have key k.\n |      \n |      Hash-partitions the resulting RDD into the given number of partitions.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 4)])\n |      &gt;&gt;&gt; y = sc.parallelize([(&#34;a&#34;, 2), (&#34;c&#34;, 8)])\n |      &gt;&gt;&gt; sorted(x.fullOuterJoin(y).collect())\n |      [(&#39;a&#39;, (1, 2)), (&#39;b&#39;, (4, None)), (&#39;c&#39;, (None, 8))]\n |  \n |  getCheckpointFile(self)\n |      Gets the name of the file to which this RDD was checkpointed\n |      \n |      Not defined if RDD is checkpointed locally.\n |  \n |  getNumPartitions(self)\n |      Returns the number of partitions in RDD\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4], 2)\n |      &gt;&gt;&gt; rdd.getNumPartitions()\n |      2\n |  \n |  getResourceProfile(self)\n |      Get the :class:`pyspark.resource.ResourceProfile` specified with this RDD or None\n |      if it wasn&#39;t specified.\n |      \n |      .. versionadded:: 3.1.0\n |      \n |      Returns\n |      -------\n |      :py:class:`pyspark.resource.ResourceProfile`\n |          The the user specified profile or None if none were specified\n |      \n |      Notes\n |      -----\n |      This API is experimental\n |  \n |  getStorageLevel(self)\n |      Get the RDD&#39;s current storage level.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; rdd1 = sc.parallelize([1,2])\n |      &gt;&gt;&gt; rdd1.getStorageLevel()\n |      StorageLevel(False, False, False, False, 1)\n |      &gt;&gt;&gt; print(rdd1.getStorageLevel())\n |      Serialized 1x Replicated\n |  \n |  glom(self)\n |      Return an RDD created by coalescing all elements within each partition\n |      into a list.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4], 2)\n |      &gt;&gt;&gt; sorted(rdd.glom().collect())\n |      [[1, 2], [3, 4]]\n |  \n |  groupBy(self, f, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f9589109670&gt;)\n |      Return an RDD of grouped items.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n |      &gt;&gt;&gt; result = rdd.groupBy(lambda x: x % 2).collect()\n |      &gt;&gt;&gt; sorted([(x, sorted(y)) for (x, y) in result])\n |      [(0, [2, 8]), (1, [1, 1, 3, 5])]\n |  \n |  groupByKey(self, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f9589109670&gt;)\n |      Group the values for each key in the RDD into a single sequence.\n |      Hash-partitions the resulting RDD with numPartitions partitions.\n |      \n |      Notes\n |      -----\n |      If you are grouping in order to perform an aggregation (such as a\n |      sum or average) over each key, using reduceByKey or aggregateByKey will\n |      provide much better performance.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; rdd = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 1), (&#34;a&#34;, 1)])\n |      &gt;&gt;&gt; sorted(rdd.groupByKey().mapValues(len).collect())\n |      [(&#39;a&#39;, 2), (&#39;b&#39;, 1)]\n |      &gt;&gt;&gt; sorted(rdd.groupByKey().mapValues(list).collect())\n |      [(&#39;a&#39;, [1, 1]), (&#39;b&#39;, [1])]\n |  \n |  groupWith(self, other, *others)\n |      Alias for cogroup but with support for multiple RDDs.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; w = sc.parallelize([(&#34;a&#34;, 5), (&#34;b&#34;, 6)])\n |      &gt;&gt;&gt; x = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 4)])\n |      &gt;&gt;&gt; y = sc.parallelize([(&#34;a&#34;, 2)])\n |      &gt;&gt;&gt; z = sc.parallelize([(&#34;b&#34;, 42)])\n |      &gt;&gt;&gt; [(x, tuple(map(list, y))) for x, y in sorted(list(w.groupWith(x, y, z).collect()))]\n |      [(&#39;a&#39;, ([5], [1], [2], [])), (&#39;b&#39;, ([6], [4], [], [42]))]\n |  \n |  histogram(self, buckets)\n |      Compute a histogram using the provided buckets. The buckets\n |      are all open to the right except for the last which is closed.\n |      e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n |      which means 1&lt;=x&lt;10, 10&lt;=x&lt;20, 20&lt;=x&lt;=50. And on the input of 1\n |      and 50 we would have a histogram of 1,0,1.\n |      \n |      If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n |      this can be switched from an O(log n) insertion to O(1) per\n |      element (where n is the number of buckets).\n |      \n |      Buckets must be sorted, not contain any duplicates, and have\n |      at least two elements.\n |      \n |      If `buckets` is a number, it will generate buckets which are\n |      evenly spaced between the minimum and maximum of the RDD. For\n |      example, if the min value is 0 and the max is 100, given `buckets`\n |      as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\n |      be at least 1. An exception is raised if the RDD contains infinity.\n |      If the elements in the RDD do not vary (max == min), a single bucket\n |      will be used.\n |      \n |      The return value is a tuple of buckets and histogram.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; rdd = sc.parallelize(range(51))\n |      &gt;&gt;&gt; rdd.histogram(2)\n |      ([0, 25, 50], [25, 26])\n |      &gt;&gt;&gt; rdd.histogram([0, 5, 25, 50])\n |      ([0, 5, 25, 50], [5, 20, 26])\n |      &gt;&gt;&gt; rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n |      ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n |      &gt;&gt;&gt; rdd = sc.parallelize([&#34;ab&#34;, &#34;ac&#34;, &#34;b&#34;, &#34;bd&#34;, &#34;ef&#34;])\n |      &gt;&gt;&gt; rdd.histogram((&#34;a&#34;, &#34;b&#34;, &#34;c&#34;))\n |      ((&#39;a&#39;, &#39;b&#39;, &#39;c&#39;), [2, 2])\n |  \n |  id(self)\n |      A unique ID for this RDD (within its SparkContext).\n |  \n |  intersection(self, other)\n |      Return the intersection of this RDD and another one. The output will\n |      not contain any duplicate elements, even if the input RDDs did.\n |      \n |      Notes\n |      -----\n |      This method performs a shuffle internally.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n |      &gt;&gt;&gt; rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n |      &gt;&gt;&gt; rdd1.intersection(rdd2).collect()\n |      [1, 2, 3]\n |  \n |  isCheckpointed(self)\n |      Return whether this RDD is checkpointed and materialized, either reliably or locally.\n |  \n |  isEmpty(self)\n |      Returns true if and only if the RDD contains no elements at all.\n |      \n |      Notes\n |      -----\n |      An RDD may be empty even when it has at least 1 partition.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; sc.parallelize([]).isEmpty()\n |      True\n |      &gt;&gt;&gt; sc.parallelize([1]).isEmpty()\n |      False\n |  \n |  isLocallyCheckpointed(self)\n |      Return whether this RDD is marked for local checkpointing.\n |      \n |      Exposed for testing.\n |  \n |  join(self, other, numPartitions=None)\n |      Return an RDD containing all pairs of elements with matching keys in\n |      `self` and `other`.\n |      \n |      Each pair of elements will be returned as a (k, (v1, v2)) tuple, where\n |      (k, v1) is in `self` and (k, v2) is in `other`.\n |      \n |      Performs a hash join across the cluster.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 4)])\n |      &gt;&gt;&gt; y = sc.parallelize([(&#34;a&#34;, 2), (&#34;a&#34;, 3)])\n |      &gt;&gt;&gt; sorted(x.join(y).collect())\n |      [(&#39;a&#39;, (1, 2)), (&#39;a&#39;, (1, 3))]\n |  \n |  keyBy(self, f)\n |      Creates tuples of the elements in this RDD by applying `f`.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)\n |      &gt;&gt;&gt; y = sc.parallelize(zip(range(0,5), range(0,5)))\n |      &gt;&gt;&gt; [(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]\n |      [(0, [[0], [0]]), (1, [[1], [1]]), (2, [[], [2]]), (3, [[], [3]]), (4, [[2], [4]])]\n |  \n |  keys(self)\n |      Return an RDD with the keys of each tuple.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; m = sc.parallelize([(1, 2), (3, 4)]).keys()\n |      &gt;&gt;&gt; m.collect()\n |      [1, 3]\n |  \n |  leftOuterJoin(self, other, numPartitions=None)\n |      Perform a left outer join of `self` and `other`.\n |      \n |      For each element (k, v) in `self`, the resulting RDD will either\n |      contain all pairs (k, (v, w)) for w in `other`, or the pair\n |      (k, (v, None)) if no elements in `other` have key k.\n |      \n |      Hash-partitions the resulting RDD into the given number of partitions.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 4)])\n |      &gt;&gt;&gt; y = sc.parallelize([(&#34;a&#34;, 2)])\n |      &gt;&gt;&gt; sorted(x.leftOuterJoin(y).collect())\n |      [(&#39;a&#39;, (1, 2)), (&#39;b&#39;, (4, None))]\n |  \n |  localCheckpoint(self)\n |      Mark this RDD for local checkpointing using Spark&#39;s existing caching layer.\n |      \n |      This method is for users who wish to truncate RDD lineages while skipping the expensive\n |      step of replicating the materialized data in a reliable distributed file system. This is\n |      useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX).\n |      \n |      Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed\n |      data is written to ephemeral local storage in the executors instead of to a reliable,\n |      fault-tolerant storage. The effect is that if an executor fails during the computation,\n |      the checkpointed data may no longer be accessible, causing an irrecoverable job failure.\n |      \n |      This is NOT safe to use with dynamic allocation, which removes executors along\n |      with their cached blocks. If you must use both features, you are advised to set\n |      `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value.\n |      \n |      The checkpoint directory set through :meth:`SparkContext.setCheckpointDir` is not used.\n |  \n |  lookup(self, key)\n |      Return the list of values in the RDD for key `key`. This operation\n |      is done efficiently if the RDD has a known partitioner by only\n |      searching the partition that the key maps to.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; l = range(1000)\n |      &gt;&gt;&gt; rdd = sc.parallelize(zip(l, l), 10)\n |      &gt;&gt;&gt; rdd.lookup(42)  # slow\n |      [42]\n |      &gt;&gt;&gt; sorted = rdd.sortByKey()\n |      &gt;&gt;&gt; sorted.lookup(42)  # fast\n |      [42]\n |      &gt;&gt;&gt; sorted.lookup(1024)\n |      []\n |      &gt;&gt;&gt; rdd2 = sc.parallelize([((&#39;a&#39;, &#39;b&#39;), &#39;c&#39;)]).groupByKey()\n |      &gt;&gt;&gt; list(rdd2.lookup((&#39;a&#39;, &#39;b&#39;))[0])\n |      [&#39;c&#39;]\n |  \n |  map(self, f, preservesPartitioning=False)\n |      Return a new RDD by applying a function to each element of this RDD.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; rdd = sc.parallelize([&#34;b&#34;, &#34;a&#34;, &#34;c&#34;])\n |      &gt;&gt;&gt; sorted(rdd.map(lambda x: (x, 1)).collect())\n |      [(&#39;a&#39;, 1), (&#39;b&#39;, 1), (&#39;c&#39;, 1)]\n |  \n |  mapPartitions(self, f, preservesPartitioning=False)\n |      Return a new RDD by applying a function to each partition of this RDD.\n\n*** WARNING: skipped 7146 bytes of output ***\n\n |      \n |       Can increase or decrease the level of parallelism in this RDD.\n |       Internally, this uses a shuffle to redistribute data.\n |       If you are decreasing the number of partitions in this RDD, consider\n |       using `coalesce`, which can avoid performing a shuffle.\n |      \n |      Examples\n |      --------\n |       &gt;&gt;&gt; rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n |       &gt;&gt;&gt; sorted(rdd.glom().collect())\n |       [[1], [2, 3], [4, 5], [6, 7]]\n |       &gt;&gt;&gt; len(rdd.repartition(2).glom().collect())\n |       2\n |       &gt;&gt;&gt; len(rdd.repartition(10).glom().collect())\n |       10\n |  \n |  repartitionAndSortWithinPartitions(self, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f9589109670&gt;, ascending=True, keyfunc=&lt;function RDD.&lt;lambda&gt; at 0x7f958904cc10&gt;)\n |      Repartition the RDD according to the given partitioner and, within each resulting partition,\n |      sort records by their keys.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n |      &gt;&gt;&gt; rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\n |      &gt;&gt;&gt; rdd2.glom().collect()\n |      [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n |  \n |  rightOuterJoin(self, other, numPartitions=None)\n |      Perform a right outer join of `self` and `other`.\n |      \n |      For each element (k, w) in `other`, the resulting RDD will either\n |      contain all pairs (k, (v, w)) for v in this, or the pair (k, (None, w))\n |      if no elements in `self` have key k.\n |      \n |      Hash-partitions the resulting RDD into the given number of partitions.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 4)])\n |      &gt;&gt;&gt; y = sc.parallelize([(&#34;a&#34;, 2)])\n |      &gt;&gt;&gt; sorted(y.rightOuterJoin(x).collect())\n |      [(&#39;a&#39;, (2, 1)), (&#39;b&#39;, (None, 4))]\n |  \n |  sample(self, withReplacement, fraction, seed=None)\n |      Return a sampled subset of this RDD.\n |      \n |      Parameters\n |      ----------\n |      withReplacement : bool\n |          can elements be sampled multiple times (replaced when sampled out)\n |      fraction : float\n |          expected size of the sample as a fraction of this RDD&#39;s size\n |          without replacement: probability that each element is chosen; fraction must be [0, 1]\n |          with replacement: expected number of times each element is chosen; fraction must be &gt;= 0\n |      seed : int, optional\n |          seed for the random number generator\n |      \n |      Notes\n |      -----\n |      This is not guaranteed to provide exactly the fraction specified of the total\n |      count of the given :class:`DataFrame`.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; rdd = sc.parallelize(range(100), 4)\n |      &gt;&gt;&gt; 6 &lt;= rdd.sample(False, 0.1, 81).count() &lt;= 14\n |      True\n |  \n |  sampleByKey(self, withReplacement, fractions, seed=None)\n |      Return a subset of this RDD sampled by key (via stratified sampling).\n |      Create a sample of this RDD using variable sampling rates for\n |      different keys as specified by fractions, a key to sampling rate map.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; fractions = {&#34;a&#34;: 0.2, &#34;b&#34;: 0.1}\n |      &gt;&gt;&gt; rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n |      &gt;&gt;&gt; sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n |      &gt;&gt;&gt; 100 &lt; len(sample[&#34;a&#34;]) &lt; 300 and 50 &lt; len(sample[&#34;b&#34;]) &lt; 150\n |      True\n |      &gt;&gt;&gt; max(sample[&#34;a&#34;]) &lt;= 999 and min(sample[&#34;a&#34;]) &gt;= 0\n |      True\n |      &gt;&gt;&gt; max(sample[&#34;b&#34;]) &lt;= 999 and min(sample[&#34;b&#34;]) &gt;= 0\n |      True\n |  \n |  sampleStdev(self)\n |      Compute the sample standard deviation of this RDD&#39;s elements (which\n |      corrects for bias in estimating the standard deviation by dividing by\n |      N-1 instead of N).\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; sc.parallelize([1, 2, 3]).sampleStdev()\n |      1.0\n |  \n |  sampleVariance(self)\n |      Compute the sample variance of this RDD&#39;s elements (which corrects\n |      for bias in estimating the variance by dividing by N-1 instead of N).\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; sc.parallelize([1, 2, 3]).sampleVariance()\n |      1.0\n |  \n |  saveAsHadoopDataset(self, conf, keyConverter=None, valueConverter=None)\n |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n |      system, using the old Hadoop OutputFormat API (mapred package). Keys/values are\n |      converted for output using either user specified converters or, by default,\n |      &#34;org.apache.spark.api.python.JavaToWritableConverter&#34;.\n |      \n |      Parameters\n |      ----------\n |      conf : dict\n |          Hadoop job configuration\n |      keyConverter : str, optional\n |          fully qualified classname of key converter (None by default)\n |      valueConverter : str, optional\n |          fully qualified classname of value converter (None by default)\n |  \n |  saveAsHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None, compressionCodecClass=None)\n |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n |      system, using the old Hadoop OutputFormat API (mapred package). Key and value types\n |      will be inferred if not specified. Keys and values are converted for output using either\n |      user specified converters or &#34;org.apache.spark.api.python.JavaToWritableConverter&#34;. The\n |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          path to Hadoop file\n |      outputFormatClass : str\n |          fully qualified classname of Hadoop OutputFormat\n |          (e.g. &#34;org.apache.hadoop.mapred.SequenceFileOutputFormat&#34;)\n |      keyClass : str, optional\n |          fully qualified classname of key Writable class\n |          (e.g. &#34;org.apache.hadoop.io.IntWritable&#34;, None by default)\n |      valueClass : str, optional\n |          fully qualified classname of value Writable class\n |          (e.g. &#34;org.apache.hadoop.io.Text&#34;, None by default)\n |      keyConverter : str, optional\n |          fully qualified classname of key converter (None by default)\n |      valueConverter : str, optional\n |          fully qualified classname of value converter (None by default)\n |      conf : dict, optional\n |          (None by default)\n |      compressionCodecClass : str\n |          fully qualified classname of the compression codec class\n |          i.e. &#34;org.apache.hadoop.io.compress.GzipCodec&#34; (None by default)\n |  \n |  saveAsNewAPIHadoopDataset(self, conf, keyConverter=None, valueConverter=None)\n |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n |      system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n |      converted for output using either user specified converters or, by default,\n |      &#34;org.apache.spark.api.python.JavaToWritableConverter&#34;.\n |      \n |      Parameters\n |      ----------\n |      conf : dict\n |          Hadoop job configuration\n |      keyConverter : str, optional\n |          fully qualified classname of key converter (None by default)\n |      valueConverter : str, optional\n |          fully qualified classname of value converter (None by default)\n |  \n |  saveAsNewAPIHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None)\n |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n |      system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n |      will be inferred if not specified. Keys and values are converted for output using either\n |      user specified converters or &#34;org.apache.spark.api.python.JavaToWritableConverter&#34;. The\n |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n |      \n |      path : str\n |          path to Hadoop file\n |      outputFormatClass : str\n |          fully qualified classname of Hadoop OutputFormat\n |          (e.g. &#34;org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat&#34;)\n |      keyClass : str, optional\n |          fully qualified classname of key Writable class\n |           (e.g. &#34;org.apache.hadoop.io.IntWritable&#34;, None by default)\n |      valueClass : str, optional\n |          fully qualified classname of value Writable class\n |          (e.g. &#34;org.apache.hadoop.io.Text&#34;, None by default)\n |      keyConverter : str, optional\n |          fully qualified classname of key converter (None by default)\n |      valueConverter : str, optional\n |          fully qualified classname of value converter (None by default)\n |      conf : dict, optional\n |          Hadoop job configuration (None by default)\n |  \n |  saveAsPickleFile(self, path, batchSize=10)\n |      Save this RDD as a SequenceFile of serialized objects. The serializer\n |      used is :class:`pyspark.serializers.PickleSerializer`, default batch size\n |      is 10.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; from tempfile import NamedTemporaryFile\n |      &gt;&gt;&gt; tmpFile = NamedTemporaryFile(delete=True)\n |      &gt;&gt;&gt; tmpFile.close()\n |      &gt;&gt;&gt; sc.parallelize([1, 2, &#39;spark&#39;, &#39;rdd&#39;]).saveAsPickleFile(tmpFile.name, 3)\n |      &gt;&gt;&gt; sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())\n |      [&#39;1&#39;, &#39;2&#39;, &#39;rdd&#39;, &#39;spark&#39;]\n |  \n |  saveAsSequenceFile(self, path, compressionCodecClass=None)\n |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n |      system, using the &#34;org.apache.hadoop.io.Writable&#34; types that we convert from the\n |      RDD&#39;s key and value types. The mechanism is as follows:\n |      \n |          1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.\n |          2. Keys and values of this Java RDD are converted to Writables and written out.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          path to sequence file\n |      compressionCodecClass : str, optional\n |          fully qualified classname of the compression codec class\n |          i.e. &#34;org.apache.hadoop.io.compress.GzipCodec&#34; (None by default)\n |  \n |  saveAsTextFile(self, path, compressionCodecClass=None)\n |      Save this RDD as a text file, using string representations of elements.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          path to text file\n |      compressionCodecClass : str, optional\n |          fully qualified classname of the compression codec class\n |          i.e. &#34;org.apache.hadoop.io.compress.GzipCodec&#34; (None by default)\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; from tempfile import NamedTemporaryFile\n |      &gt;&gt;&gt; tempFile = NamedTemporaryFile(delete=True)\n |      &gt;&gt;&gt; tempFile.close()\n |      &gt;&gt;&gt; sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\n |      &gt;&gt;&gt; from fileinput import input\n |      &gt;&gt;&gt; from glob import glob\n |      &gt;&gt;&gt; &#39;&#39;.join(sorted(input(glob(tempFile.name + &#34;/part-0000*&#34;))))\n |      &#39;0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n&#39;\n |      \n |      Empty lines are tolerated when saving to text files.\n |      \n |      &gt;&gt;&gt; from tempfile import NamedTemporaryFile\n |      &gt;&gt;&gt; tempFile2 = NamedTemporaryFile(delete=True)\n |      &gt;&gt;&gt; tempFile2.close()\n |      &gt;&gt;&gt; sc.parallelize([&#39;&#39;, &#39;foo&#39;, &#39;&#39;, &#39;bar&#39;, &#39;&#39;]).saveAsTextFile(tempFile2.name)\n |      &gt;&gt;&gt; &#39;&#39;.join(sorted(input(glob(tempFile2.name + &#34;/part-0000*&#34;))))\n |      &#39;\\n\\n\\nbar\\nfoo\\n&#39;\n |      \n |      Using compressionCodecClass\n |      \n |      &gt;&gt;&gt; from tempfile import NamedTemporaryFile\n |      &gt;&gt;&gt; tempFile3 = NamedTemporaryFile(delete=True)\n |      &gt;&gt;&gt; tempFile3.close()\n |      &gt;&gt;&gt; codec = &#34;org.apache.hadoop.io.compress.GzipCodec&#34;\n |      &gt;&gt;&gt; sc.parallelize([&#39;foo&#39;, &#39;bar&#39;]).saveAsTextFile(tempFile3.name, codec)\n |      &gt;&gt;&gt; from fileinput import input, hook_compressed\n |      &gt;&gt;&gt; result = sorted(input(glob(tempFile3.name + &#34;/part*.gz&#34;), openhook=hook_compressed))\n |      &gt;&gt;&gt; b&#39;&#39;.join(result).decode(&#39;utf-8&#39;)\n |      &#39;bar\\nfoo\\n&#39;\n |  \n |  setName(self, name)\n |      Assign a name to this RDD.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; rdd1 = sc.parallelize([1, 2])\n |      &gt;&gt;&gt; rdd1.setName(&#39;RDD1&#39;).name()\n |      &#39;RDD1&#39;\n |  \n |  sortBy(self, keyfunc, ascending=True, numPartitions=None)\n |      Sorts this RDD by the given keyfunc\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; tmp = [(&#39;a&#39;, 1), (&#39;b&#39;, 2), (&#39;1&#39;, 3), (&#39;d&#39;, 4), (&#39;2&#39;, 5)]\n |      &gt;&gt;&gt; sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n |      [(&#39;1&#39;, 3), (&#39;2&#39;, 5), (&#39;a&#39;, 1), (&#39;b&#39;, 2), (&#39;d&#39;, 4)]\n |      &gt;&gt;&gt; sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n |      [(&#39;a&#39;, 1), (&#39;b&#39;, 2), (&#39;1&#39;, 3), (&#39;d&#39;, 4), (&#39;2&#39;, 5)]\n |  \n |  sortByKey(self, ascending=True, numPartitions=None, keyfunc=&lt;function RDD.&lt;lambda&gt; at 0x7f958904cd30&gt;)\n |      Sorts this RDD, which is assumed to consist of (key, value) pairs.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; tmp = [(&#39;a&#39;, 1), (&#39;b&#39;, 2), (&#39;1&#39;, 3), (&#39;d&#39;, 4), (&#39;2&#39;, 5)]\n |      &gt;&gt;&gt; sc.parallelize(tmp).sortByKey().first()\n |      (&#39;1&#39;, 3)\n |      &gt;&gt;&gt; sc.parallelize(tmp).sortByKey(True, 1).collect()\n |      [(&#39;1&#39;, 3), (&#39;2&#39;, 5), (&#39;a&#39;, 1), (&#39;b&#39;, 2), (&#39;d&#39;, 4)]\n |      &gt;&gt;&gt; sc.parallelize(tmp).sortByKey(True, 2).collect()\n |      [(&#39;1&#39;, 3), (&#39;2&#39;, 5), (&#39;a&#39;, 1), (&#39;b&#39;, 2), (&#39;d&#39;, 4)]\n |      &gt;&gt;&gt; tmp2 = [(&#39;Mary&#39;, 1), (&#39;had&#39;, 2), (&#39;a&#39;, 3), (&#39;little&#39;, 4), (&#39;lamb&#39;, 5)]\n |      &gt;&gt;&gt; tmp2.extend([(&#39;whose&#39;, 6), (&#39;fleece&#39;, 7), (&#39;was&#39;, 8), (&#39;white&#39;, 9)])\n |      &gt;&gt;&gt; sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n |      [(&#39;a&#39;, 3), (&#39;fleece&#39;, 7), (&#39;had&#39;, 2), (&#39;lamb&#39;, 5),...(&#39;white&#39;, 9), (&#39;whose&#39;, 6)]\n |  \n |  stats(self)\n |      Return a :class:`StatCounter` object that captures the mean, variance\n |      and count of the RDD&#39;s elements in one operation.\n |  \n |  stdev(self)\n |      Compute the standard deviation of this RDD&#39;s elements.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; sc.parallelize([1, 2, 3]).stdev()\n |      0.816...\n |  \n |  subtract(self, other, numPartitions=None)\n |      Return each value in `self` that is not contained in `other`.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 4), (&#34;b&#34;, 5), (&#34;a&#34;, 3)])\n |      &gt;&gt;&gt; y = sc.parallelize([(&#34;a&#34;, 3), (&#34;c&#34;, None)])\n |      &gt;&gt;&gt; sorted(x.subtract(y).collect())\n |      [(&#39;a&#39;, 1), (&#39;b&#39;, 4), (&#39;b&#39;, 5)]\n |  \n |  subtractByKey(self, other, numPartitions=None)\n |      Return each (key, value) pair in `self` that has no pair with matching\n |      key in `other`.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = sc.parallelize([(&#34;a&#34;, 1), (&#34;b&#34;, 4), (&#34;b&#34;, 5), (&#34;a&#34;, 2)])\n |      &gt;&gt;&gt; y = sc.parallelize([(&#34;a&#34;, 3), (&#34;c&#34;, None)])\n |      &gt;&gt;&gt; sorted(x.subtractByKey(y).collect())\n |      [(&#39;b&#39;, 4), (&#39;b&#39;, 5)]\n |  \n |  sum(self)\n |      Add up the elements in this RDD.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; sc.parallelize([1.0, 2.0, 3.0]).sum()\n |      6.0\n |  \n |  sumApprox(self, timeout, confidence=0.95)\n |      Approximate operation to return the sum within a timeout\n |      or meet the confidence.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; rdd = sc.parallelize(range(1000), 10)\n |      &gt;&gt;&gt; r = sum(range(1000))\n |      &gt;&gt;&gt; abs(rdd.sumApprox(1000) - r) / r &lt; 0.05\n |      True\n |  \n |  take(self, num)\n |      Take the first num elements of the RDD.\n |      \n |      It works by first scanning one partition, and use the results from\n |      that partition to estimate the number of additional partitions needed\n |      to satisfy the limit.\n |      \n |      Translated from the Scala implementation in RDD#take().\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver&#39;s memory.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n |      [2, 3]\n |      &gt;&gt;&gt; sc.parallelize([2, 3, 4, 5, 6]).take(10)\n |      [2, 3, 4, 5, 6]\n |      &gt;&gt;&gt; sc.parallelize(range(100), 100).filter(lambda x: x &gt; 90).take(3)\n |      [91, 92, 93]\n |  \n |  takeOrdered(self, num, key=None)\n |      Get the N elements from an RDD ordered in ascending order or as\n |      specified by the optional key function.\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver&#39;s memory.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n |      [1, 2, 3, 4, 5, 6]\n |      &gt;&gt;&gt; sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n |      [10, 9, 7, 6, 5, 4]\n |  \n |  takeSample(self, withReplacement, num, seed=None)\n |      Return a fixed-size sampled subset of this RDD.\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver&#39;s memory.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; rdd = sc.parallelize(range(0, 10))\n |      &gt;&gt;&gt; len(rdd.takeSample(True, 20, 1))\n |      20\n |      &gt;&gt;&gt; len(rdd.takeSample(False, 5, 2))\n |      5\n |      &gt;&gt;&gt; len(rdd.takeSample(False, 15, 3))\n |      10\n |  \n |  toDF(self, schema=None, sampleRatio=None)\n |      Converts current :class:`RDD` into a :class:`DataFrame`\n |      \n |      This is a shorthand for ``spark.createDataFrame(rdd, schema, sampleRatio)``\n |      \n |      Parameters\n |      ----------\n |      schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n |          a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n |          column names, default is None.  The data type string format equals to\n |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n |          omit the ``struct&lt;&gt;`` and atomic types use ``typeName()`` as their format, e.g. use\n |          ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\n |          We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\n |      sampleRatio : float, optional\n |          the sample ratio of rows used for inferring\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; rdd.toDF().collect()\n |      [Row(name=&#39;Alice&#39;, age=1)]\n |  \n |  toDebugString(self)\n |      A description of this RDD and its recursive dependencies for debugging.\n |  \n |  toLocalIterator(self, prefetchPartitions=False)\n |      Return an iterator that contains all of the elements in this RDD.\n |      The iterator will consume as much memory as the largest partition in this RDD.\n |      With prefetch it may consume up to the memory of the 2 largest partitions.\n |      \n |      Parameters\n |      ----------\n |      prefetchPartitions : bool, optional\n |          If Spark should pre-fetch the next partition\n |          before it is needed.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; rdd = sc.parallelize(range(10))\n |      &gt;&gt;&gt; [x for x in rdd.toLocalIterator()]\n |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n |  \n |  top(self, num, key=None)\n |      Get the top N elements from an RDD.\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver&#39;s memory.\n |      \n |      It returns the list sorted in descending order.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; sc.parallelize([10, 4, 2, 12, 3]).top(1)\n |      [12]\n |      &gt;&gt;&gt; sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n |      [6, 5]\n |      &gt;&gt;&gt; sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n |      [4, 3, 2]\n |  \n |  treeAggregate(self, zeroValue, seqOp, combOp, depth=2)\n |      Aggregates the elements of this RDD in a multi-level tree\n |      pattern.\n |      \n |      depth : int, optional\n |          suggested depth of the tree (default: 2)\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; add = lambda x, y: x + y\n |      &gt;&gt;&gt; rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n |      &gt;&gt;&gt; rdd.treeAggregate(0, add, add)\n |      -5\n |      &gt;&gt;&gt; rdd.treeAggregate(0, add, add, 1)\n |      -5\n |      &gt;&gt;&gt; rdd.treeAggregate(0, add, add, 2)\n |      -5\n |      &gt;&gt;&gt; rdd.treeAggregate(0, add, add, 5)\n |      -5\n |      &gt;&gt;&gt; rdd.treeAggregate(0, add, add, 10)\n |      -5\n |  \n |  treeReduce(self, f, depth=2)\n |      Reduces the elements of this RDD in a multi-level tree pattern.\n |      \n |      Parameters\n |      ----------\n |      f : function\n |      depth : int, optional\n |          suggested depth of the tree (default: 2)\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; add = lambda x, y: x + y\n |      &gt;&gt;&gt; rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n |      &gt;&gt;&gt; rdd.treeReduce(add)\n |      -5\n |      &gt;&gt;&gt; rdd.treeReduce(add, 1)\n |      -5\n |      &gt;&gt;&gt; rdd.treeReduce(add, 2)\n |      -5\n |      &gt;&gt;&gt; rdd.treeReduce(add, 5)\n |      -5\n |      &gt;&gt;&gt; rdd.treeReduce(add, 10)\n |      -5\n |  \n |  union(self, other)\n |      Return the union of this RDD and another one.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; rdd = sc.parallelize([1, 1, 2, 3])\n |      &gt;&gt;&gt; rdd.union(rdd).collect()\n |      [1, 1, 2, 3, 1, 1, 2, 3]\n |  \n |  unpersist(self, blocking=False)\n |      Mark the RDD as non-persistent, and remove all blocks for it from\n |      memory and disk.\n |      \n |      .. versionchanged:: 3.0.0\n |         Added optional argument `blocking` to specify whether to block until all\n |         blocks are deleted.\n |  \n |  values(self)\n |      Return an RDD with the values of each tuple.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; m = sc.parallelize([(1, 2), (3, 4)]).values()\n |      &gt;&gt;&gt; m.collect()\n |      [2, 4]\n |  \n |  variance(self)\n |      Compute the variance of this RDD&#39;s elements.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; sc.parallelize([1, 2, 3]).variance()\n |      0.666...\n |  \n |  withResources(self, profile)\n |      Specify a :class:`pyspark.resource.ResourceProfile` to use when calculating this RDD.\n |      This is only supported on certain cluster managers and currently requires dynamic\n |      allocation to be enabled. It will result in new executors with the resources specified\n |      being acquired to calculate the RDD.\n |      \n |      .. versionadded:: 3.1.0\n |      \n |      Notes\n |      -----\n |      This API is experimental\n |  \n |  zip(self, other)\n |      Zips this RDD with another one, returning key-value pairs with the\n |      first element in each RDD second element in each RDD, etc. Assumes\n |      that the two RDDs have the same number of partitions and the same\n |      number of elements in each partition (e.g. one was made through\n |      a map on the other).\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; x = sc.parallelize(range(0,5))\n |      &gt;&gt;&gt; y = sc.parallelize(range(1000, 1005))\n |      &gt;&gt;&gt; x.zip(y).collect()\n |      [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n |  \n |  zipWithIndex(self)\n |      Zips this RDD with its element indices.\n |      \n |      The ordering is first based on the partition index and then the\n |      ordering of items within each partition. So the first item in\n |      the first partition gets index 0, and the last item in the last\n |      partition receives the largest index.\n |      \n |      This method needs to trigger a spark job when this RDD contains\n |      more than one partitions.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; sc.parallelize([&#34;a&#34;, &#34;b&#34;, &#34;c&#34;, &#34;d&#34;], 3).zipWithIndex().collect()\n |      [(&#39;a&#39;, 0), (&#39;b&#39;, 1), (&#39;c&#39;, 2), (&#39;d&#39;, 3)]\n |  \n |  zipWithUniqueId(self)\n |      Zips this RDD with generated unique Long ids.\n |      \n |      Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n |      n is the number of partitions. So there may exist gaps, but this\n |      method won&#39;t trigger a spark job, which is different from\n |      :meth:`zipWithIndex`.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; sc.parallelize([&#34;a&#34;, &#34;b&#34;, &#34;c&#34;, &#34;d&#34;, &#34;e&#34;], 3).zipWithUniqueId().collect()\n |      [(&#39;a&#39;, 0), (&#39;b&#39;, 1), (&#39;c&#39;, 4), (&#39;d&#39;, 2), (&#39;e&#39;, 5)]\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties defined here:\n |  \n |  context\n |      The :class:`SparkContext` that this RDD was created on.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "help(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4851d74d-8869-409a-ac10-26542846db19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Basic Actions-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02f5d320-d036-4d1c-8b86-7cb28795a746",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">the take action to number list [1, 2, 3, 4, 5, 6, 7, 8]\n",
       "the take action to cloud list of x [&#39;azure&#39;, &#39;aws&#39;]\n",
       "the take action to cloud list of y [&#39;azure&#39;, &#39;aws&#39;, &#39;google cloud&#39;]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">the take action to number list [1, 2, 3, 4, 5, 6, 7, 8]\nthe take action to cloud list of x [&#39;azure&#39;, &#39;aws&#39;]\nthe take action to cloud list of y [&#39;azure&#39;, &#39;aws&#39;, &#39;google cloud&#39;]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"the take action to number list\", rdd_1.take(8))\n",
    "print(\"the take action to cloud list of x\", x.take(2))\n",
    "print(\"the take action to cloud list of y\", y.take(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69b2ec7a-3129-440b-b9f9-a636036a5b91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">the top action to number list [20, 19, 18, 17, 16, 15, 14, 13]\n",
       "the top action to cloud list of x [&#39;google cloud&#39;, &#39;gcp&#39;]\n",
       "the top action to cloud list of y [&#39;google cloud&#39;, &#39;gcp&#39;, &#39;azure&#39;]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">the top action to number list [20, 19, 18, 17, 16, 15, 14, 13]\nthe top action to cloud list of x [&#39;google cloud&#39;, &#39;gcp&#39;]\nthe top action to cloud list of y [&#39;google cloud&#39;, &#39;gcp&#39;, &#39;azure&#39;]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"the top action to number list\", rdd_1.top(8))\n",
    "print(\"the top action to cloud list of x\", x.top(2))\n",
    "print(\"the top action to cloud list of y\", y.top(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36ce797e-b6d3-45a9-949a-c9fc138a20c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">count of number list 20\n",
       "count of cloud list of x 7\n",
       "count of cloud list of y 4\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">count of number list 20\ncount of cloud list of x 7\ncount of cloud list of y 4\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"count of number list\", rdd_1.count())\n",
    "print(\"count of cloud list of x\", x.count())\n",
    "print(\"count of cloud list of y\", y.count()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "978785db-32ac-47e3-b588-e89eccfb0bf4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">mean of number list 10.5\n",
       "standard deviation of number list 5.766281297335398\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">mean of number list 10.5\nstandard deviation of number list 5.766281297335398\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"mean of number list\", rdd_1.mean())\n",
    "print(\"standard deviation of number list\", rdd_1.stdev())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21d83a50-d7d3-489d-8af8-e7e8bbec0b27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">taking sample from number list [10, 7, 15, 6, 15]\n",
       "taking sample from number list [9, 8, 14, 12, 16]\n",
       "taking sample from cloud list of y [&#39;gcp&#39;, &#39;azure&#39;, &#39;google cloud&#39;, &#39;aws&#39;]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">taking sample from number list [10, 7, 15, 6, 15]\ntaking sample from number list [9, 8, 14, 12, 16]\ntaking sample from cloud list of y [&#39;gcp&#39;, &#39;azure&#39;, &#39;google cloud&#39;, &#39;aws&#39;]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"taking sample from number list\", rdd_1.takeSample(True,5))\n",
    "print(\"taking sample from number list\", rdd_1.takeSample(False,5))\n",
    "print(\"taking sample from cloud list of y\", y.takeSample(False,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8e9e930-2cdb-45eb-a5cf-0f0cfdad7037",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[24]: 210</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[24]: 210</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd_1.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "508d5036-96d2-4ae9-8434-7483c07c3710",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[26]: 20</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[26]: 20</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd_1.reduce(lambda x,y: x if x>y else y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf39ba5a-8c9c-435c-84bf-b2555efbb384",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd_2 = sc.parallelize([1,2,3,1,1,1,2,3,4,5,2,4,5,3,3,3,3,5,5,3,2,2,1,1,2,3,4,5,2,4,5,3,3,3,3,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ce29911-c0dc-4b63-8145-f8e61c5d4bac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[33]: defaultdict(int, {1: 6, 2: 7, 3: 12, 4: 4, 5: 7})</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[33]: defaultdict(int, {1: 6, 2: 7, 3: 12, 4: 4, 5: 7})</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd_2.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22619a88-8c76-4337-93b2-7c43e86947a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[40]: defaultdict(int, {&#39;a&#39;: 4, &#39;b&#39;: 2, &#39;c&#39;: 2, &#39;d&#39;: 1})</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[40]: defaultdict(int, {&#39;a&#39;: 4, &#39;b&#39;: 2, &#39;c&#39;: 2, &#39;d&#39;: 1})</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_1 = ([(\"a\",1), (\"b\",2), (\"c\",3), (\"a\",1), (\"a\",1), (\"a\",2), (\"b\",3), (\"c\",4), (\"d\",5)])\n",
    "rdd_3 = sc.parallelize(data_1)\n",
    "rdd_3.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f21bb8e-e1c4-43f2-b01e-2be1d9dc17dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[46]: 14</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[46]: 14</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd_1.fold(4, lambda x, y : y-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65abf335-93f2-40d9-a786-b6333cd7afa5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[47]: (count: 36, mean: 2.9722222222222223, stdev: 1.322583984124157, max: 5.0, min: 1.0)</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[47]: (count: 36, mean: 2.9722222222222223, stdev: 1.322583984124157, max: 5.0, min: 1.0)</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd_2.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eeac023-7633-4acf-a544-6cc19185c475",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[48]: 1.7492283950617282</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[48]: 1.7492283950617282</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd_2.variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6a2d303-69ca-44a6-a85f-104c4ac0b3c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[49]: 1.799206349206349</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[49]: 1.799206349206349</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd_2.sampleVariance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08387ae6-e1fc-48a2-bee1-fe6066e48fd8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "RDD transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e48ee8d3-f521-4e15-81b1-a1461a0820e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[8]: [22, 42, 62, 82, 104, 126, 146, 166, 186]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[8]: [22, 42, 62, 82, 104, 126, 146, 166, 186]</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#map\n",
    "data_1 = [11,21,31,41,52,63,73,83,93]\n",
    "rdd_2 = sc.parallelize(data_1)\n",
    "map_1 = rdd_2.map(lambda x:x*2)\n",
    "map_1.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "196b2c0b-46be-49ef-939e-7b001a6102b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[0.11, 0.21, 0.31, 0.41, 0.52, 0.63, 0.73, 0.83, 0.93]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">[0.11, 0.21, 0.31, 0.41, 0.52, 0.63, 0.73, 0.83, 0.93]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def divison(x):\n",
    "    return x/100\n",
    "map_2 = rdd_2.map(divison)\n",
    "print(map_2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9c3548c-e7aa-400b-89c1-796376df109e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[11, 21, 31, 41, 52, 63, 73, 83, 93]\n",
       "[52]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">[11, 21, 31, 41, 52, 63, 73, 83, 93]\n[52]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filter\n",
    "map_4 = rdd_2.filter(lambda x:x%2==1)\n",
    "print(rdd_2.collect())\n",
    "print(map_4.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "662684c2-cc6d-4613-ad8b-40ee931bb121",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[51, 102, 61, 122, 71, 142, 81, 162, 91, 182, 101, 202, 111, 222, 121, 242, 131, 262]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">[51, 102, 61, 122, 71, 142, 81, 162, 91, 182, 101, 202, 111, 222, 121, 242, 131, 262]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#flat map\n",
    "new_rdd = sc.parallelize([51,61,71,81,91,101,111,121,131])\n",
    "flat_map = new_rdd.flatMap(lambda x: (x,x*2))\n",
    "print(flat_map.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed505d6f-70ae-499d-a316-e09e3cd84bfc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">the example for map is [[&#39;google&#39;, &#39;cloud&#39;], [&#39;azure&#39;, &#39;cloud&#39;], [&#39;amazon&#39;, &#39;web&#39;, &#39;services&#39;], [&#39;IBM&#39;], [&#39;oracle&#39;, &#39;cloud&#39;]]\n",
       "the example for flat map is [&#39;google&#39;, &#39;cloud&#39;, &#39;azure&#39;, &#39;cloud&#39;, &#39;amazon&#39;, &#39;web&#39;, &#39;services&#39;, &#39;IBM&#39;, &#39;oracle&#39;, &#39;cloud&#39;]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">the example for map is [[&#39;google&#39;, &#39;cloud&#39;], [&#39;azure&#39;, &#39;cloud&#39;], [&#39;amazon&#39;, &#39;web&#39;, &#39;services&#39;], [&#39;IBM&#39;], [&#39;oracle&#39;, &#39;cloud&#39;]]\nthe example for flat map is [&#39;google&#39;, &#39;cloud&#39;, &#39;azure&#39;, &#39;cloud&#39;, &#39;amazon&#39;, &#39;web&#39;, &#39;services&#39;, &#39;IBM&#39;, &#39;oracle&#39;, &#39;cloud&#39;]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# faltmap vs map\n",
    "services = [\"google cloud\", \"azure cloud\", \"amazon web services\", \"IBM\", \"oracle cloud\"]\n",
    "companies_rdd = sc.parallelize(services)\n",
    "companies_flat_map = companies_rdd.flatMap(lambda word: word.split(\" \"))\n",
    "companies_flat = companies_rdd.map(lambda word: word.split(\" \"))\n",
    "print(f\"the example for map is {companies_flat.collect()}\")\n",
    "print(f\"the example for flat map is {companies_flat_map.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f12d20d5-d827-499d-bc07-84579f0825fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">&lt;generator object sum_1 at 0x7fa9497ef120&gt;\n",
       "15\n",
       "40\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">&lt;generator object sum_1 at 0x7fa9497ef120&gt;\n15\n40\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [[1,2,3,4,5],[6,7,8,9,10]]\n",
    "def sum_1(iterator):\n",
    "    for element in iterator:\n",
    "        if isinstance(element, list):\n",
    "            yield sum(element)\n",
    "        else:\n",
    "            yield element\n",
    "print(sum_1(data))\n",
    "for x in sum_1(data):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3de984d-fa96-4bd9-8a08-38a4d4d7ef5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">15\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">15\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [[1,2,3,4,5],[6,7,8,9,10]]\n",
    "def sum_11(iterator):\n",
    "    for element in iterator:\n",
    "        if isinstance(element, list):\n",
    "            return sum(element)\n",
    "        else:\n",
    "            return element\n",
    "print(sum_11(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f81c24a3-0176-46d5-ad1f-1efe3fbfac91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[15, 40]\n",
       "[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">[15, 40]\n[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mappartitions\n",
    "data = [1,2,3,4,5,6,7,8,9,10]\n",
    "mp_rdd = sc.parallelize(data, 2)\n",
    "def sum_map_partitions(iterator):\n",
    "    yield sum(iterator)\n",
    "\n",
    "partitions_sum =mp_rdd.mapPartitions(sum_map_partitions)\n",
    "print(partitions_sum.collect())\n",
    "print(mp_rdd.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9182bf25-0eff-4a31-9542-e7c7c5449f8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">&lt;generator object sum_11 at 0x7fa955950510&gt;\n",
       "[15, 40]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">&lt;generator object sum_11 at 0x7fa955950510&gt;\n[15, 40]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_old = [[1,2,3,4,5],[6,7,8,9,10]]\n",
    "old_rdd = sc.parallelize(data_old, 2)\n",
    "def sum_11(iterator):\n",
    "    for element in iterator:\n",
    "        if isinstance(element, list):\n",
    "            yield sum(element)\n",
    "        else:\n",
    "            yield element\n",
    "print(sum_11(data))\n",
    "old_partitions = old_rdd.mapPartitions(sum_11)\n",
    "print(old_partitions.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dfb2819-e1f3-432f-8241-f27f54e6259d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">the index is: 0, and the value 2\n",
       "the index is: 0, and the value 4\n",
       "the index is: 0, and the value 6\n",
       "the index is: 0, and the value 8\n",
       "the index is: 0, and the value 10\n",
       "the index is: 1, and the value 12\n",
       "the index is: 1, and the value 14\n",
       "the index is: 1, and the value 16\n",
       "the index is: 1, and the value 18\n",
       "the index is: 1, and the value 20\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">the index is: 0, and the value 2\nthe index is: 0, and the value 4\nthe index is: 0, and the value 6\nthe index is: 0, and the value 8\nthe index is: 0, and the value 10\nthe index is: 1, and the value 12\nthe index is: 1, and the value 14\nthe index is: 1, and the value 16\nthe index is: 1, and the value 18\nthe index is: 1, and the value 20\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# map partitoin with index\n",
    "def sum_11(partion_index, iterator):\n",
    "    for element in iterator:\n",
    "        if isinstance(element, list):\n",
    "            for x in element:\n",
    "                yield (f\"the index is: {partion_index}, and the value {x*2}\")\n",
    "        else:\n",
    "            yield (f\"the index is: {partion_index}, and the value is{element *2}\")\n",
    "mp_iter_index = old_rdd.mapPartitionsWithIndex(sum_11)\n",
    "for x in mp_iter_index.collect():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64448026-6901-4b3e-b871-5400c96279a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[2, 3, 4, 5, 6, 7, 10]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">[2, 3, 4, 5, 6, 7, 10]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sample\n",
    "data = [1,2,3,4,5,6,7,8,9,10]\n",
    "data_rdd = sc.parallelize(data)\n",
    "x= data_rdd.sample(False, 0.7)\n",
    "print(x.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af48d2fc-5bee-4bed-af14-e28f31055dc8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">x is [1, 2, 3, 4, 5, 6, 7, 8, 7, 8, 9, 10, 11, 12, 13, 14]\n",
       "y is [7, 8, 9, 10, 11, 12, 13, 14, 1, 2, 3, 4, 5, 6, 7, 8]\n",
       "&lt;class &#39;list&#39;&gt;\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">x is [1, 2, 3, 4, 5, 6, 7, 8, 7, 8, 9, 10, 11, 12, 13, 14]\ny is [7, 8, 9, 10, 11, 12, 13, 14, 1, 2, 3, 4, 5, 6, 7, 8]\n&lt;class &#39;list&#39;&gt;\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#union\n",
    "rdd_00 = sc.parallelize([1,2,3,4,5,6,7,8])\n",
    "rdd_01 = sc.parallelize([7,8,9,10,11,12,13,14])\n",
    "x = rdd_00.union(rdd_01).collect()\n",
    "y = rdd_01.union(rdd_00).collect()\n",
    "print(f\"x is {x}\")\n",
    "print(f\"y is {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b556b7e-eab4-42eb-b2f9-fb1f7d698861",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">r is [8, 7]\n",
       "s is [8, 7]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">r is [8, 7]\ns is [8, 7]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# intersection\n",
    "r = rdd_00.intersection(rdd_01).collect()\n",
    "s = rdd_01.intersection(rdd_00).collect()\n",
    "print(f\"r is {r}\")\n",
    "print(f\"s is {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82b3b7de-26e5-4eed-b82c-faa5c3106139",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">the distinct values are [4, 8, 12, 1, 5, 9, 13, 2, 6, 10, 14, 3, 7, 11]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">the distinct values are [4, 8, 12, 1, 5, 9, 13, 2, 6, 10, 14, 3, 7, 11]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#distinct\n",
    "x_1 = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 7, 8, 9, 10, 11, 12, 13, 14])\n",
    "y = x.distinct()\n",
    "z = y.collect()\n",
    "print(f\"the distinct values are {z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "281101d8-69ac-4087-b105-15e665b8aaee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">key b:[5, 2, 3]\n",
       "key a:[1, 2, 3]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">key b:[5, 2, 3]\nkey a:[1, 2, 3]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# group by key\n",
    "data_key = [(\"a\", 1), (\"b\", 5), (\"a\", 2), (\"b\", 2), (\"a\", 3), (\"b\", 3)]\n",
    "rdd_key = sc.parallelize(data_key)\n",
    "q = rdd_key.groupByKey().collect()\n",
    "for key, values in q:\n",
    "    print(f\"key {key}:{list(values)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04f1b2c8-bff6-4f2e-8b2b-fbb6997e50aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">the data type of board cast is &lt;class &#39;pyspark.broadcast.Broadcast&#39;&gt;\n",
       "the boardcast values are [1, 2, 3, 4, 5, 6, 7]\n",
       "37\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">the data type of board cast is &lt;class &#39;pyspark.broadcast.Broadcast&#39;&gt;\nthe boardcast values are [1, 2, 3, 4, 5, 6, 7]\n37\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [1,2,3,4,5,6,7]\n",
    "broadcast_v = sc.broadcast(data)\n",
    "print(\"the data type of board cast is\",type(broadcast_v))\n",
    "print(\"the boardcast values are\", broadcast_v.value)\n",
    "accum = sc.accumulator(9)\n",
    "x =sc.parallelize(data).foreach(lambda x: accum.add(x))\n",
    "print(accum.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84a9c57f-c30a-423b-ba51-a90c80a0714f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">The result of join is [(&#39;a&#39;, (2, 8)), (&#39;a&#39;, (2, 6)), (&#39;a&#39;, (1, 8)), (&#39;a&#39;, (1, 6)), (&#39;b&#39;, (3, 7))]\n",
       "The result of right join is  [(&#39;a&#39;, (2, 8)), (&#39;a&#39;, (2, 6)), (&#39;a&#39;, (1, 8)), (&#39;a&#39;, (1, 6)), (&#39;b&#39;, (3, 7)), (&#39;d&#39;, (None, 5))]\n",
       "The result of left join is  [(&#39;a&#39;, (2, 8)), (&#39;a&#39;, (2, 6)), (&#39;a&#39;, (1, 8)), (&#39;a&#39;, (1, 6)), (&#39;c&#39;, (4, None)), (&#39;b&#39;, (3, 7))]\n",
       "The result of full join is  [(&#39;a&#39;, (2, 8)), (&#39;a&#39;, (2, 6)), (&#39;a&#39;, (1, 8)), (&#39;a&#39;, (1, 6)), (&#39;c&#39;, (4, None)), (&#39;b&#39;, (3, 7)), (&#39;d&#39;, (None, 5))]\n",
       "the result for cross join is  [((&#39;c&#39;, 4), (&#39;a&#39;, 8)), ((&#39;c&#39;, 4), (&#39;b&#39;, 7)), ((&#39;c&#39;, 4), (&#39;a&#39;, 6)), ((&#39;c&#39;, 4), (&#39;d&#39;, 5)), ((&#39;b&#39;, 3), (&#39;a&#39;, 8)), ((&#39;b&#39;, 3), (&#39;b&#39;, 7)), ((&#39;b&#39;, 3), (&#39;a&#39;, 6)), ((&#39;b&#39;, 3), (&#39;d&#39;, 5)), ((&#39;a&#39;, 2), (&#39;a&#39;, 8)), ((&#39;a&#39;, 2), (&#39;b&#39;, 7)), ((&#39;a&#39;, 2), (&#39;a&#39;, 6)), ((&#39;a&#39;, 2), (&#39;d&#39;, 5)), ((&#39;a&#39;, 1), (&#39;a&#39;, 8)), ((&#39;a&#39;, 1), (&#39;b&#39;, 7)), ((&#39;a&#39;, 1), (&#39;a&#39;, 6)), ((&#39;a&#39;, 1), (&#39;d&#39;, 5))]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">The result of join is [(&#39;a&#39;, (2, 8)), (&#39;a&#39;, (2, 6)), (&#39;a&#39;, (1, 8)), (&#39;a&#39;, (1, 6)), (&#39;b&#39;, (3, 7))]\nThe result of right join is  [(&#39;a&#39;, (2, 8)), (&#39;a&#39;, (2, 6)), (&#39;a&#39;, (1, 8)), (&#39;a&#39;, (1, 6)), (&#39;b&#39;, (3, 7)), (&#39;d&#39;, (None, 5))]\nThe result of left join is  [(&#39;a&#39;, (2, 8)), (&#39;a&#39;, (2, 6)), (&#39;a&#39;, (1, 8)), (&#39;a&#39;, (1, 6)), (&#39;c&#39;, (4, None)), (&#39;b&#39;, (3, 7))]\nThe result of full join is  [(&#39;a&#39;, (2, 8)), (&#39;a&#39;, (2, 6)), (&#39;a&#39;, (1, 8)), (&#39;a&#39;, (1, 6)), (&#39;c&#39;, (4, None)), (&#39;b&#39;, (3, 7)), (&#39;d&#39;, (None, 5))]\nthe result for cross join is  [((&#39;c&#39;, 4), (&#39;a&#39;, 8)), ((&#39;c&#39;, 4), (&#39;b&#39;, 7)), ((&#39;c&#39;, 4), (&#39;a&#39;, 6)), ((&#39;c&#39;, 4), (&#39;d&#39;, 5)), ((&#39;b&#39;, 3), (&#39;a&#39;, 8)), ((&#39;b&#39;, 3), (&#39;b&#39;, 7)), ((&#39;b&#39;, 3), (&#39;a&#39;, 6)), ((&#39;b&#39;, 3), (&#39;d&#39;, 5)), ((&#39;a&#39;, 2), (&#39;a&#39;, 8)), ((&#39;a&#39;, 2), (&#39;b&#39;, 7)), ((&#39;a&#39;, 2), (&#39;a&#39;, 6)), ((&#39;a&#39;, 2), (&#39;d&#39;, 5)), ((&#39;a&#39;, 1), (&#39;a&#39;, 8)), ((&#39;a&#39;, 1), (&#39;b&#39;, 7)), ((&#39;a&#39;, 1), (&#39;a&#39;, 6)), ((&#39;a&#39;, 1), (&#39;d&#39;, 5))]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# join\n",
    "a_rdd = sc.parallelize([(\"c\",4),(\"b\",3),(\"a\",2),(\"a\",1)])\n",
    "b_rdd = sc.parallelize([(\"a\",8),(\"b\",7),(\"a\",6),(\"d\",5)])\n",
    "join = a_rdd.join(b_rdd)\n",
    "print(\"The result of join is\",join.collect())\n",
    "right_join = a_rdd.rightOuterJoin(b_rdd)\n",
    "print(\"The result of right join is \", right_join.collect()) \n",
    "left_join = a_rdd.leftOuterJoin(b_rdd)\n",
    "print(\"The result of left join is \", left_join.collect())\n",
    "full_join = a_rdd.fullOuterJoin(b_rdd)\n",
    "print(\"The result of full join is \", full_join.collect())\n",
    "cross_join = a_rdd.cartesian(b_rdd)\n",
    "print(\"the result for cross join is \" ,cross_join.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c4aa012-e71c-4902-867f-4993ed35b9f8",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark rdd 2024-06-23 04:26:21",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
